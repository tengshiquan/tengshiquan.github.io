---
layout:     post
title:      Regret Matching & CFR
subtitle:   
date:       2020-04-19 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-dice.jpg"
catalog: true
tags:
    - AI
    - Imperfect Information
    - Game Theory 
    - CFR

---



# Regret Matching & CFR 算法

问题, 对gametree 超级大的, 知道对方的策略, 怎么求纯策略的br. 或者求 可利用性.



**RegretMatching (Normal) => CFR (Extensive)**  

根据regret sum不停的修正当前策略, 最后的**平均策略**收敛到均衡.    

Regret Matching的核心： regret概念 , 下一轮策略，平均策略。



CFR 核心概念：   reach prob 到某h，Counterfactual value （$h$ 和 $I$ 的）

CFR 从root开始,  前向传播reach prob, 后向传播EV; 对每个node遍历求各个action的regret , 要算上cfr prob, 然后 RegretMatching得到下一次策略. 

**CFR, 对某个h, 就是在那个node, 玩家没有选a,  而是执行$\sigma$造成的regret,  然后乘以cfr权重:其他人执行$\sigma$ 到这个node的几率. 其他人不愿意走这边,意义不大**



**chance-sampling** , 相比 **Vanilla** CFR 对chance节点全遍历 , 使用的是MC采样的方式.



编程的时候要注意的一点： 如果分开迭代两个玩家的策略比较简单， 或者 在收益函数中， 把当前视角的玩家id传入； 如果放一起，则每轮的收益的视角是变化的，一个办法是每次*-1 ， 另外一个办法是在 统一按照玩家1，到算玩家2regret的时候，在再sum上乘以-1 





## Regret Matching

Regret 遗憾， 表示一轮game中，选a 相比另外一个策略造成的 收益差 ， 所以R是一个action的向量。

**Regret** ： $$R^{T}\left(s_{a}\right)=\sum_{t=1}^{T}\left(v^{t}(a)-v^{t}\right)$$   **T轮<mark>收益差</mark>的sum**    ， 这里是拿每轮选a 与 每轮采用的策略($S_t$ 可以每轮不一样) 的累计收益差 
其中， $$ v^{t}=\sum_{a \in A} \sigma^{t}(a) v^{t}(a) $$   即当前t轮的策略中 每个action的分量*每个action的收益， 就是第t轮策略的预期收益。

**Regret Matching** :     $$ R_{+}^{t}(a)=\max \left\{0, R^{t}(a)\right\} $$       regret的取正
$$
\sigma^{t+1}(a)=\frac{R_{+}^{t}(a)}{\sum_{a^{\prime} \in A} R_{+}^{t}\left(a^{\prime}\right)}
$$





2000年，Hart和Mas-Colell提出--**Regret Matching**。 

这个时候是针对 normal-form games。 首先， 什么是 regret =  u(not take) - u(real)

For action profile $a \in A$ let $s_{i}$ be player $i$ 's action and $s_{-i}$ be the actions of all other players. Further, let $u\left(s_{i}^{\prime}, s_{-i}\right)$ be the utility of an **action profile** with $s_{i}^{\prime}$ substituted for $s_{i},$ i.e. the utility if player $i$ had played $s_{i}^{\prime}$ in place of $s_{i} .$ Then, after the play, player $i$ 's **regret** for not having played $s_{i}^{\prime}$ is $u\left(s_{i}^{\prime}, s_{-i}\right)-u(a) .$ Note that this is 0 when $s_{i}^{\prime}=s_{i}$
For this example, we regret not having played paper $u$ (paper, paper) $-u($ rock, paper $)=0-(-1)=1$ and we regret not having played scissors $u($ scissors, paper $)-u($ rock, paper $)= +1-(-1)=2$ 







由定义， regret 可以是负数。所以下面有 nonnegative counterfactual regret $R_{i}^{T,+}(I, a)=\max \left(R_{i}^{T}(I, a), 0\right)$



**regret matching** :  agents 选择action 的分布与 regret+(**正的**) 成正比 proportional to **positive** regrets.  即 regret+越大, 被选中几率越大;  

如猜拳, 收益为赢了1, 输了-1, 平0;  第一轮随机出:   A出锤, B出布;  计算regret的时候，B的行为固定，A的遗憾为: 
1. 没选布 : $u(布, 布)−u(锤,布) = 1$  
2. 没选剪刀:  $u(剪, 剪)−u(锤,布) = 2$  

则第二轮A的策略更新, 几率为: 锤:0, 布: 1/3, 剪: 2/3.  分母 3 是所有regret 的总和.   
如果第二轮, A出剪刀,  B出锤, 则本轮regret为 锤 1, 布 2, 剪 0     
累积**cumulative regrets**为  锤 1, 布 3, 剪 2 , 策略更新为  锤:1/6, 布: 3/6, 剪: 2/6

<mark>当前策略就是 </mark> **normalized positive regrets**: $ \frac{\text{Regret}^+(a)}{\sum(\text{Regret}
^+)} $,  分子是**该action** 到第T轮次一直**累加**的regret sum，**无论正负**都累加起来，再取非负得到 Regret+(a)；分母是**所有action**的Regret+ ;  得到当前T+1轮次的策略 ；   即 regret 求和， 再取非负部分求策略。

**这里一个点，是每个轮次的reget加起来，再取正，还是先取正再加起来。应该是先加起来再取正。code的时候，可以只留一个累加sum，用的时候取正； 同时有RM+算法，cumulative 变量每次加上新的收益后都取正，**

对于**regret 的取正**，当某把赢了对方的时候， regret就是负数，但最后被抹去了成为0了，所以求纳什均衡，就不是寻求的赢的方法，而是不输。

最终求平均策略， 就是把每次的策略加起来再除以T！但显然程序中保存所有的历史策略太占内存。只需保留一个 P(a)的sum即可， 长度为$\|a\|$的数组。



下面的算法, 可以通过**self-play**将预期regret最小化:

- For each player,  初始化 cumulative regrets 为 0.
- For some number of iterations:
  - Compute a **regret-matching** strategy profile. (如果没有正的,则uniform随机)
  - Add the strategy profile to the strategy profile sum.     sum用于计算最后的输出策略
  - Select each player action profile according the strategy profile.  **下一步策略，  下一步选某action的几率是基于regret sum**,  即，当前策略是基于sum regret matching
  - Compute player regrets. 需要遍历当前节点的所有其他action.
  - Add player regrets to player cumulative regrets.
  - Return the **average strategy profile**, i.e. the **strategy profile sum divided by the number of iterations**.    **平均策略就是把每个轮次的策略, $p^t(a)$加起来, 再除以迭代总次数**







核心,  regret的sum的, 然后找出正的部分, 生成下轮策略, 最后所有策略相加平均,求得average strategy across all iterations.  **平均策略** 最终会**收敛**到 **correlated equilibrium 相关均衡**.  

The **average strategy** that is computed by regret matching is the strategy that **minimizes regret against the opponent’s fixed strategy**. In other words, it is a **best response** to their strategy. 是br之一,但不是纯策略br.



**Folk Theorem**: In a two-player zero-sum game, if $\frac{R^{T}}{T} \leq \epsilon$ for both players, then their **average strategies** over the $T$ iterations form a $2 \varepsilon$ -Nash equilibrium.    **Folk该定理建立了 regret 与 纳什均衡的 联系**。 



#### Regret Matching

- **Idea**: Pick actions in proportion to the amount of **positive** regret on the actions
- Let  $R^{T}(a)=\sum_{t=0}^{T}\left(u^{t}(a)-u^{t}\right)$      ，  regret 求和是无论正负的， 就是把这个action好的坏的都统计出来， 这个可能随着T增大而无限增大
- $$p^{T+1}(a)=\frac{\max \left\{R^{T}(a), 0\right\}}{\sum_{a^{\prime} \in A} \max \left\{R^{T}\left(a^{\prime}\right), 0\right\}}$$   ， 下一步策略，是统计之前所有 regret sum+ 的结果，因为这个是求几率， 几率不会有负数的。
- Start with a uniform random strategy  ， 开始是一个均匀的随机策略
- **Theorem**: If we pick actions according to Regret Matching (RM) then $$\max _{a \in A} \frac{R^{T}(a)}{T} \leq \frac{\Delta \sqrt{|A|}}{\sqrt{T}}$$
  [Hart and Mas-Colell 2000]  

该定理配合 folk 定理，  $R^{T}(a)$ 可能是无限的， 但除以T以后， 是有个上限， 趋近于0， 说明收敛到 近似纳什均衡策略。





```java
//Get current mixed strategy through regret-matching
private double[] getStrategy() {
	double normalizingSum = 0;
	for (int a = 0; a < NUM_ACTIONS; a++) {
		strategy[a] = regretSum[a] > 0 ? regretSum[a] : 0; // 用regret+
		normalizingSum += strategy[a];
	}
	for (int a = 0; a < NUM_ACTIONS; a++) {
		if (normalizingSum > 0)
			strategy[a] /= normalizingSum;
        else
            strategy[a] = 1.0 / NUM_ACTIONS;
            strategySum[a] += strategy[a];  // 一直在累计 每轮策略action的分量，用于计算平均策略 ， 最后除以总次数T即可
        }
	return strategy;
}


//Get average mixed strategy across all training iterations
public double[] getAverageStrategy() {
    double[] avgStrategy = new double[NUM_ACTIONS];
    double normalizingSum = 0;
    for (int a = 0; a < NUM_ACTIONS; a++)
    	normalizingSum += strategySum[a];  //这里写的有点问题， 这个normalizingSum应该等于总次数T ， 直接使用总的次数T即可
    for (int a = 0; a < NUM_ACTIONS; a++)
        if (normalizingSum > 0)
        	avgStrategy[a] = strategySum[a] / normalizingSum;
        else
        	avgStrategy[a] = 1.0 / NUM_ACTIONS;
    return avgStrategy;
}
```









#### Proof of Convergence to Nash Equilibrium

<mark>**Folk Theorem**</mark>: In a two-player zero-sum game, if $\frac{R^{T}}{T} \leq \epsilon$ for both players, then their **average strategies** over the $T$ iterations form a $2 \varepsilon$ Nash equilibrium. 

Proof:
Let $u_{i}$ be utility for player $i$ and $u_{-i}$ be utility for the opponent 
Let $\sigma_{i}^{t}$ be the strategy chosen by $i$ on iteration
$$
\begin{array}{l}
\left\{\max _{a_{i}^{\prime} \in A_{i}} \sum_{t \leq T}\left(u_{i}\left(a_{i}^{\prime}, \sigma_{-i}^{t}\right)-u_{i}\left(\sigma_{i}^{t}, \sigma_{-i}^{t}\right)\right)\right\} \leq \epsilon T \\
\max _{a_{i}^{\prime} \in A_{i}} T\left(u_{i}\left(a_{i}^{\prime}, \bar{\sigma}_{-i}^{T}\right)\right)-\sum_{t \leq T} u_{i}\left(\sigma_{i}^{t}, \sigma_{-i}^{t}\right) \leq \epsilon T
\end{array}
$$

上面花括号里面的是Regret ,  $\bar{\sigma}_{-i}^{T}$ 是平均策略. 

since $u_{1}(\sigma)=-u_{2}(\sigma),$ so we sum the above for both players:

$$
\max _{a_{1}^{\prime} \in A_{1}} T\left(u_{1}\left(a_{1}^{\prime}, \bar{\sigma}_{2}^{T}\right)\right)+\max _{a_{2}^{\prime} \in A_{2}} T\left(u_{2}\left(\bar{\sigma}_{1}^{T}, a_{2}^{\prime}\right)\right) \leq 2 \epsilon T
$$

$$
\max _{a_{1}^{1} \in A_{1}} T\left(u_{1}\left(a_{1}^{\prime}, \bar{\sigma}_{2}^{T}\right)\right)-\min _{a_{2}^{\prime} \in A_{2}} T\left(u_{1}\left(\bar{\sigma}_{1}^{T}, a_{2}^{\prime}\right)\right) \leq 2 \epsilon T
$$

since $$\min _{a_{2}^{\prime} \in A_{2}} u_{1}\left(a_{2}^{\prime}, \bar{\sigma}_{1}^{T}\right) \leq u_{1}\left(\bar{\sigma}_{1}^{T}, \bar{\sigma}_{2}^{T}\right)$$  ,  the theorem holds











## Counterfactual Regret Minimization

将Regret Minimization**扩展**到 信息不完全 **序列博弈sequential games**.     
可被转为 one-time-action normal-form game,即 静态博弈.  但是一般用 game tree 来representation.



#### Kuhn Poker

类似德州的超级简化版游戏.  一共有三张牌1,2,3; 玩家A和B各拿一张私牌。只有bet, pass

<img src="/img/2020-04-19-CFR-Intro.assets/image-20200515005455936.png"   style="zoom: 33%;" />

Kuhn Poker 有 12 个 information sets.   ( 1, 2, 3, b1,b2,b3, p1, p2, p3, 1pb, 2pb, 3pb ) 



#### CFR

扩展必须考虑两点: 1. 玩家的策略到达每个信息集到达的概率   2. 怎么通过信息集反向传递收益 . 

**CFR基本思想: 在迭代training中, 对递归访问到的每一个信息集，使用regret matching求混合策略.** 

**从上往下, 传播reach概率; 从下往上, 传播预期收益.** 

就是一个node, 如果N个action,则看当前策略在该node的EV,(即各个子node的ev乘以策略中各个action的概率), 然后自己采取某个action, 即取某个action到达的子node的EV, 差值就是regret, 再乘以cfr prob.

还有就是, 对于gametree, 全遍历一次, 相当于每个信息集里面的每个node也都遍历了一次, 他们的权重由到达概率确定, 因为由对方的策略来定. 显然, 越上层的信息集, 被访问的次数越多. 相当于其上的加权平均.



另外要注意, 因为前期两个人的策略已经使得到达有些node的reach 是0, 所以这些节点的CFV就是0, CFR自然也是0, 平均策略就变成了随机策略, 也说明该算法求的是纳什均衡策略, 不是对各个点都是很积极的去争取胜利, 如果突然出现到了一个状态, 是对自己有利的, 但均衡策略也只是让自己不输. 是一个**消极保守策略**.  其实这个node可以直接减掉,因为是严劣的.





Notation:

- $A$  ,  set of all game actions

- $I$ ,   an information set

- $A(I)$  ,  set of legal actions for information set $I$ 

- $t$ and $T$ ,   time steps

- $\sigma_{i}^{t}$ ,  player $i$ 's **strategy** , 映射,   ( $I_{i}$ ,  $a \in A\left(I_{i}\right)$  )  =>   probability choose $a$ in $I_{i}$ ,  at  $t$

- $\sigma_{-i}$ ,   **strategy profile** excludes player $i$ 

- $\sigma_{I \rightarrow a}$  ,  与 $\sigma$ 等价,  除了在信息集 $I$ 上总是选 $a$ 

- $h$ , **history** ,  sequence of actions (included chance outcomes) ,  (root, ...)

- $\pi^{\sigma}(h)$ , **<mark>reach probability</mark>** of  $h$ with $\sigma$  

- $\pi^{\sigma}(I)$ ,  **reach probability** of $I$ through **all possible histories** $h$ in $I,$ $\pi^{\sigma}(I)=\sum_{h \in I} \pi^{\sigma}(h)$ .

- $\pi_{-i}^{\sigma}(I)$ , **<mark>counterfactual reach probability</mark>** of $I$ with $\sigma$, except that, we treat current player $i$ actions to reach the state as having probability 1.  表示到达 这个infoset $I$ 的概率, 其中玩家$i$ 的action都是为了到达这个infoset $I$ . 相当于假设玩家i一路有目地选过来.

- **counterfactual** : player i’s strategy was modified to have intentionally played to $I_i$ , **虚拟, 指该玩家的策略被改为,玩家i有意走到某个信息集, 那么一路上自己的选择的都是确定的,几率都是1; 有点像是复盘, 所以是虚拟**

- $Z$ , all terminal game histories ,  **(root,..., leaf)**  ， 从头到尾的 历史

- $h \sqsubset z$ , **proper prefix** , nonterminal game history  真前序

- $u_{i}(z)$ , **utility** 

- **<mark>counterfactual value</mark>**: at nonterminal $h$ , $$v_{i}(\sigma, h)=\sum_{z \in Z, h \sqsubset z} \pi_{-i}^{\sigma}(h) \pi^{\sigma}(h, z) u_{i}(z)$$ ;
  
    $$v_{i}(\sigma, h)= \pi_{-i}^{\sigma}(h) \sum_{z \in Z, h \sqsubset z}  \pi^{\sigma}(h, z) u_{i}(z)$$  reach prob可以提到前面
  $\pi_{-i}^{\sigma}(h)$ 表示玩家想走到 $h$ 的概率,  也即其他人的按照-i的策略走到h的几率， 之后则是按照策略组合$\sigma$ 走到结束,  这一subgame求期望收益
  
- $I$上的可以记为 : $$v_{i}(\sigma, I)=\sum_{z \in Z_{I}} \pi_{-i}^{\sigma}(z[I]) \pi^{\sigma}(z[I], z) u_{i}(z)$$ ，  $z[I]$  是某个z 的前缀并且在$I$ 中 , 这个系数无法提到前面

- **<mark>counterfactual regret</mark>**: not taking $a$ at history $h$ , $$r(h, a)=v_{i}\left(\sigma_{I \rightarrow a}, h\right)-v_{i}(\sigma, h)$$ 

- **counterfactual regret**: not taking $a$ at  $I$ , $$r(I, a)=\sum_{h \in I} r(h, a)$$  ;    
  **在信息集上的regret, 因为一个信息集里面会有若干node, 这些node的cfr prob是不一样, 所以相当于一个权重**
  
- **cumulative CFR** : $$R_{i}^{T}(I, a)=\sum_{t=1}^{T} r_{i}^{t}(I, a)$$ ,  **累计**的， 某个信息集$I$ 上的策略换成a之后的总regret      这里没有算 average的

- nonnegative CFR ,  $$R_{i}^{T,+}(I, a)=\max \left(R_{i}^{T}(I, a), 0\right)$$



![](/img/2020-04-19-CFR-Intro.assets/image-20200827221913002.png)

看这个图， 相当于考虑了因果性， 只考虑在某个node下（其实是某些个node，某个infoset），只改一步a造成的改进。 因为是全遍历倒推， 所以这个改进是必然的.   然后计算 regret的时候， 要考虑走到当前node下， 其他人的策略造成的概率影响， 即所有的外部影响！！注意， 这里不考虑自己策略造成的到达该node的概率的影响！！



**CFR, 对某个h, 就是在那个node, 玩家没有选a, 而是执行$\sigma$造成的regret, 然后乘以cfr权重:其他人执行$\sigma$ 到这个node的几率.**

**为什么要去掉玩家i的概率, 因为在玩家i的点, 玩家i对自己的信息是完全掌握的, 所以不存在概率的情况, 玩家i完全可以凭自己就走到某个信息集, 乘以其他人的概率.**

将regret-matching 应用到上面的 正CFR , 得到 策略更新公式:   当前action的累计regret/所有累计regret

$$
\sigma_{i}^{T+1}(I, a)=\left\{\begin{array}{ll}
\frac{R_{i}^{T,+}(I, a)}{\sum_{a \in A(I)} R_{i}^{T,+}(I, a)} & \text { if } \sum_{a \in A(I)} R_{i}^{T,+}(I, a)>0 \\
\frac{1}{|A(I)|} & \text { otherwise }
\end{array}\right.
$$



对每个$I$ , 用该公式计算 cumulative CFR ;  对每个action, CFR生成下一个state, 然后通过递归计算每个action的收益.   然后反向递推.  类似于, 带几率的动态规划.

$\bar{\sigma}^{T}$ , **average strategy profile** at $I$,  approaches an equilibrium as $T \rightarrow \infty .$    
$\bar{\sigma}^{T}(I)$,  average strategy at information set $I$, is obtained by normalizing $s_{I}$ over all actions $a \in A(I)$.  

average strategy profile converges to a Nash equilibrium, not the final strategy profile.  平均策略收敛

定义玩家i从时间1到T的**平均策略 average strategy** $$\bar{\sigma}_{i}^{t}$$ . 特别的, 对每个信息集  $$I \in \mathcal{I}_{i}$$ , 对每个$a \in A(I)$,   **<mark>注意, 这里求平均策略,要乘以 自己的 reach prob.</mark> 很多代码里都是直接求平均**

$$
\bar{\sigma}_{i}^{t}(I)(a)=\frac{\sum_{t=1}^{T} \pi_{i}^{\sigma^{t}}(I) \sigma^{t}(I)(a)}{\sum_{t=1}^{T} \pi_{i}^{\sigma^{t}}(I)}
$$

这里, 因为每轮策略$\sigma$ 是迭代更新的, 所以策略有个时间t的版本上标 $\sigma^t$ , 所以也就有了平均策略的概念,  即总体上采取a的平均概率. 





<img src="/img/2020-04-19-CFR-Intro.assets/image-20200515031233598.png"  style="zoom:80%;" />

第26行是记录，当前轮次，这个node对应的 infoset $I$ 下的 a 对应的policy。 用于最终求平均策略。 

论文有 Kuhn Poker 的代码,  特别要注意的是，在调用cfr之前洗牌，而不是在递归调用cfr的过程中处理chance事件，chance节点的结果可以预先采样。通常情况下，这样做更容易和直接，所以洗牌取代了伪代码第8到10行的if条件。这种形式的蒙特卡洛式采样被称为 "**chance-sampling**"，尽管有趣的是，CFR可以完全不进行采样（"**Vanilla CFR**"），也可以使用许多不同形式的采样方案[9]。



> regret的思路与博弈论中的纳什均衡的思路是匹配的:  纳什均衡要的是不输, 不亏, 对赢多少没要求.  对0和博弈, 均衡收益就是0 , 所以只要不亏, 或者亏的极少, 就是接近均衡了.









```java
cfr(cards, "", 1, 1);

double cfr(int[] cards, String history, double p0, double p1) {
//⟨Return payoff for terminal states⟩
       if (terminal) {
            return payoff
        }
 		String infoSet = cards[player] + history;
//⟨Get information set node or create it if nonexistant⟩
		Node node = nodeMap.get(infoSet);
  
//递归求期望回报以及遍历到的各个节点的regret ⟨For each action, recursively call cfr with additional history and probability⟩
        for (int a = 0; a < NUM_ACTIONS; a++) {
            String nextHistory = history + (a == 0 ? "p" : "b");
            util[a] = player == 0
                    ? - cfr(cards, nextHistory, p0 * strategy[a], p1)
                    : - cfr(cards, nextHistory, p0, p1 * strategy[a]);
            nodeUtil += strategy[a] * util[a];  //  求当前策略的 期望回报
        }
// ⟨For each action, compute and accumulate counterfactual regret⟩
        for (int a = 0; a < NUM_ACTIONS; a++) {
            double regret = util[a] - nodeUtil;
            node.regretSum[a] += (player == 0 ? p1 : p0) * regret;  // 当前的 regret
        }
        return nodeUtil;  // 返回 之后的 期望回报
    }
```



##### CFR Pseudocode

~~~js
function CFR(node, reach[2])  //Call CFR(Initial, {1, 1}) at the start of each iteration, for as many iterations as you wish.
    if (node is terminal) then
        return {node.value[0] ⋅ reach[1], node.value[1] ⋅ reach[0]}
    else 
        ev[2] = 0
        if (node is chance) then
            for each action in node:
                new_reach[2] = {reach[0] ⋅ prob(action), reach[1] ⋅ prob(action)}
                ev += CFR(node.do_action(action), new_reach)
        else
            action_ev[number of actions][2] = 0
            player = node.whose_turn, opponent = 1 – player
            set probabilities for actions in proportion to positive regret
            for each action in node:
                node.stored_strategy[action] += reach[player] ⋅ prob[action]  //update the average strat (normalize at the end)
                new_reach[player] = reach[player] ⋅ prob[action], new_reach[opponent] = reach[opponent]
                action_ev[action] = CFR(node.do_action(action), new_reach)  //get the value for taking this action
                ev[player] += prob[action] ⋅ action_ev[action][player]
                ev[opponent] += action_ev[opponent]
            for each action in node:
                node.regret[action] += (action_ev[action][player] – ev[player])  //update the regret for each action
        return ev
~~~

##### Alternating-Updates CFR Pseudocode

```js
function CFR(node, reach, p)  //Call CFR(Initial, 1, p) at the start of each traversal, where p alternates between P1 and P2.
    if (node is terminal) then
        return node.value[p] ⋅ reach
    else
        ev = 0
        if (node is chance) then
            for each action in node:
                ev += CFR(node.do_action(action), reach * prob[action], p)
        else
            set probabilities for actions in proportion to positive regret (if it is the first time this information set is encountered this traversal)
            if (node.whose_turn == p):
                for each action in node:
                    action_ev[action] = CFR(node.do_action(action), reach, p)  //get the value for taking this action
                    ev += prob[action] ⋅ action_ev[action]
                for each action in node:
                    node.regret[action] += (action_ev[action] – ev)  //update the regret for each action
            else
                for each action in node:
                    node.stored_strategy[action] += reach * prob[action]  //update the average strat (normalize at the end)
                    ev += CFR(node.do_action(action), reach * prob[action], p)
        return ev

```

两个玩家交替算CFR的.  相当于一个fix, 一个improve. 但慢慢都会improve到均衡.











### Fixed-Strategy Iteration CFR Minimization

针对某些博弈的改进.   imperfect recall

#### Dudo

类似酒吧的骰子游戏. 不过有1作为通配.

每位玩家围坐在一张桌子旁，开始时有五个骰子。骰子是随着每轮游戏的进行而丢掉的，游戏的目标是最后剩下最后一个有骰子的玩家。每轮游戏开始时，所有玩家同时掷出一次骰子，并仔细观察自己掷出的骰子，同时不让其他人看。开始的玩家对玩家们集体掷出的骰子进行宣称claim，顺时针方向的玩家依次进行宣称，每个人要么做出更有力的宣称，要么挑战前一个宣称，宣布 "Dudo"（西班牙语 "我怀疑 "的意思）。一轮挑战结束，玩家展示自己的骰子，参与挑战的两位玩家中的一位失去骰子。输掉的骰子被放在玩家的视野中。

n × r

宣称由一个正数的骰子和这些骰子的等级组成，例如两个5、7个3或两个1。在 Dudo 中，1 的等级是wild的, 万能配.  一般来说，如果有rank和/或骰子数量的增加，一个宣称比另一个宣称更强。也就是说，比如说，一个2×4的主张可能会在2×6（等级增加）或4×3（数量增加）之后。这个一般规则的例外是关于等级1。rank 1 的数目要翻倍。所以在rank要求排序中，1×1、2×1和3×1分别在2×2、4×2和6×2之前。

游戏从回合开始的玩家按顺时针方向进行，每个宣称严格增加，直到有一名玩家用 "Dudo "挑战前一名。这时，所有的杯子都会被举起. 

比如说，假设安、鲍勃和卡尔正在玩，卡尔挑战鲍勃声称的7×6。有三种可能的结果。
- 实际rank数超过了宣称。在这种情况下，挑战者输掉的骰子数等于实际等级数和要求数之间的差额。举例说明。算上6和1，实际数是10。因此，作为一个错误的挑战者，Cal失去了10 - 7 = 3个骰子。
- 实际rank数小于宣称。在这种情况下，被挑战者损失的骰子数等于个数差。举例说明。算上6的和1的，实际数是5，因此，作为被挑战者，鲍勃失去了7 - 5 = 2个骰子。
- 实际的rank数与被挑战者的要求相等。在这种情况下，除了被挑战的玩家，其他玩家都会输掉一个骰子。举例说明。算上6的和1的，实际数确实如Bob声称的那样是7。在这个特殊情况下，安和卡尔各输1个骰子，以奖励鲍勃的准确主张。



#### Imperfect Recall of Actions

more recent claims, more important to the decision at hand.  这里用的abstraction形式是简化 information sets , **recall** up to $m$ previous claims. 

For example, claim sequence: 1×5, 2×5, 4×2, 5×4, 3×1, 6×4, 7×2.  m = 3,  information set : {},{1×5},{1×5;2×5},{1×5;2×5;4×2},...,{3×1;6×4;7×2}. 

**对某些游戏, 最近的history足够做决策了.** 

Imperfect recall 可以缩小 计算规模.  如图。 在CFR[10]中，有一些安全的抽象技术可以在保证收敛到均衡的，但遗憾的是，上述抽象技术并不满足所需条件。因此，**收敛到纳什均衡可能已经不可能**了。然而，对Dudo、Poker和其他游戏的实验表明，可以应用imperfect recall，只需在收敛率和游戏性能上受到轻微的惩罚（与节省的内存相比）。

![](/img/2020-04-19-CFR-Intro.assets/image-20200516135129782.png)

一个重要而有趣的结果，即用Imperfect recall进行抽象。可以有不同的路径通往同一个抽象的信息集。因此，CFR 会在递归中多次访问同一信息集。例如，假设我们玩的是两个人的dudo，m = 3  
s1 : 1×2, 1×3, 1×5, 1×6, 2×2   
s2 :                   1×5, 1×6, 2×2    
这两个是同一个信息集, 因为前面两个会被遗忘掉. 

此外，假设在这两种情况下都有相同的结果，那么从这一点上看，在这两种情况下，给定两个玩家所使用的策略组合的预期收益将是相同的，因为确定回报的唯一重要的是最近的2×2的主张。CFR将枚举这两种序列。FSICFR通过重组树状遍历和CFR的计算，避免了冗余的计算。



##### Liar Die

玩家1先投了一個S面的骰子，不给其他人看, 並宣称rank。然后，对手2有两种可能的动作。
- 怀疑。玩家1开,  如果rank的等级大于或等于声称的等级，玩家1获胜。否则，对手2获胜。
- 接受。 对手2接受,  玩家1不开，直接将骰子交给对手2，对手2在掷骰子并自己观察后，必须提出比前一次的要求高的宣称。这样，选手们将轮流偷偷地掷骰子，连续不断地提出更高的宣称，直到有一位选手怀疑。

这个游戏, recall m=2就足够了,  因为骰子会重新roll.



一般来说，假设我们可以提前固定训练迭代的所有偶然事件。对于有限博弈，这就会在所有可能的信息集的所有可能路径上产生一个**定向无周期图directed acyclic graph (DAG)**。考虑图2随着深度的增加，通向一个节点的可能路径的数量呈指数级增长。如果我们将DAG模式继续向更深的方向发展，节点的线性增长和路径的指数级增长。  
如果我们应用递归CFR，这种路径的指数级增长都会出现在Dudo和Liar Die中。随着每一次CFR访问一个信息集节点，我们都会进行一次后悔更新，这可能会影响下一次CFR访问该节点的可能的混合策略。从表面上看，我们似乎受制于对同一节点进行这些指数级增长的访问。此外，除非我们刻意引入随机化来选择行动的排序，否则这些访问历史的排序可能会对累积的CFR产生有趣的波动和流动。
最重要的是，我们希望有一些训练手段可以避免对节点的访问量呈指数级增长，同时保留CFR最小化的好处。

<img src="/img/2020-04-19-CFR-Intro.assets/image-20200516150421470.png"  style="zoom:50%;" />



这样的游戏，确实可以通过对拓扑结构排序的定向无周期图中的每个节点进行一次前向和后向访问来执行regret最小化。递归CFR被重组为一个动态规划，其中累积regret和策略在每个训练迭代中更新一次。 **之前是遍历gametree, 通过遍历树上的节点来递归cfr.**   由于节点策略在整个玩家到达/实现概率信息的前向传播过程中保持固定，我们将该算法称为**Fixed-Strategy Iteration Counterfactual Regret Minimization (FSICFR)**。与CFR一样，它依赖于**regret-matching**方程收敛到纳什均衡。

**从本质上说，CFR遍历了大量的游戏子树，在保持历史记录的同时，用每个玩家会到达每个节点（即信息集）的概率向前传递(从root到leaf)，并反向传递收益，用于更新父节点的regret，从而更新下一轮的策略。**
FSICFR将CFR算法分为两次迭代传递，一次前向传递，一次后向传递，通过节点的DAG。在前向传递中，每个玩家的访问次数和到达概率都是累积的，但所有的策略都是固定的。(**相比之下，在CFR中，CFR每一次访问node，node上的策略都会被更新**），在所有的访问次数和概率累积后，后向传递计算效用和更新regret。FSICFR时间复杂度 正比于 节点数乘以平均节点出度outdegree，而CFR复杂度 正比于 节点访问次数乘以平均节点出度outdegree。 由于在这种问题中，节点访问的次数是指数级增长的，所以相对于CFR，FSICFR在每一次训练迭代中的计算时间相对于CFR来说是指数级的节省。

<img src="/img/2020-04-19-CFR-Intro.assets/image-20200516153050051.png"   />

 

对于这种简单的、零和形式的FSICFR，需要一个重要的、特定领域的假设：玩家节点和抽象的信息集之间有一对一的对应关系，我们对节点的访问必须包含足够多的状态信息（例如，预定的公共和私人信息，对双方玩家来说），这样就可以选择合适的后继节点。 而对于Dudo来说，这意味着算法预设了玩家的玩发，知道哪些玩家信息集是合法的后继。



#### Interlude: Thresholding Actions and Resetting Strategy Sums

最初，每一个动作都有一个统一的概率，所以在平均策略中总是会有至少一些动作的概率。此外，累积CFR可能需要一些时间来收敛，所以我们往往会有一个小概率的行动，即使是那些明显错误的行动，我们也会有一个小概率。

一种 "净化 "或净化策略的方法是设置一个行动概率阈值（如0.001），将任何行动的概率低于这个阈值的概率归零。

另一种获得均衡策略的更好的近似度的方法是干脆不包括早期迭代的策略。例如，可以在给定的早期迭代次数或分数后，将所有策略总和重置为零。观察将策略总和重置为零的结果。



### Exploiting Mistakes

##### Epsilon Error Modeling

在这些计算手段中可以加入的一个假设是，假设棋手将以固定的约束误差策略下棋。一个简单的模型假设，在小概率ε的情况下，玩家将随机选择一步棋。在所有其他时间，玩家将遵循我们通常的后悔匹配策略。这本质上代表了统一策略和后悔匹配策略的混合。

对于一个给定的节点，可以用代数法来建立模型，即分别计算出统一策略效用和后悔匹配效用。ε代数收益 是(1-ε)乘以后悔匹配策略效用加上ε乘以统一策略效用。

 $u_{i, \epsilon}(h)=(1-\epsilon) u_{i, \sigma}(h)+\epsilon u_{i, r}(h)$ :

$$
u_{i, \sigma}(h)=\left\{\begin{array}{ll}u_{i}(h) & \text { if } h \in Z ; \\ \sum_{a \in A(h)} \sigma(h, a) u_{i, \epsilon}(h a) & \text { otherwise }\end{array} \quad \text { and } u_{r}(h)=\left\{\begin{array}{ll}u_{i}(h) & \text { if } h \in Z \\ \sum_{a \in A(h)} \frac{1}{|A(h)|} u_{i, \epsilon}(h a) & \text { otherwise }\end{array}\right.\right.
$$


The problem is that the number of terms grows exponentially with the number of actions remaining from $h$ to a leaf. To see this more clearly, imagine a fixed-depth horizon version of the recursive definition above which falls back back to the old utilities after some depth $d .$ Then, assuming a fixed branching factor of $ \vert A \vert :$ when $d=1$, there would be $2 \vert A \vert $ child terms. When $d=2,$ there would be $4 \vert A \vert ^{2}$ terms. At depth $d,$ there would be $(2 \vert A \vert )^{d}$ terms. This places a significant computational burden for algebraic representation since it accounts for every possible perturbation in the future.

一个更简单的方法是不以代数方式处理误差，而是以一定的概率引入误差。





## Reference

An Introduction to Counterfactual Regret Minimization

















