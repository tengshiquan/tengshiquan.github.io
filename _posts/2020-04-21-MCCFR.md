---
layout:     post
title:      Monte Carlo CFR
subtitle:   Monte Carlo Sampling for Regret Minimization in Extensive Games
date:       2020-04-21 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-poker.jpg"
catalog: true
tags:
    - AI
    - CFR
    - Imperfect Information
    - Alberta
    - Texas
    - Game Theory

---



## Monte Carlo Sampling for Regret Minimization in Extensive Games

2009   from  alberta

核心思想： 每个结局$z$之前都是每次遍历都会访问到， 现在抽样访问，几率是q， 则r/q ，要求采样后期望不变 ；


- **vanilla CFR**  对所有节点全遍历

- **chance-sampled CFR**  CS chance节点采样,  其他全遍历  

- **Outcome-Sampling MCCFR**   OS   chance sample,  对两个玩家, 按照各自策略sample , 无分叉
- **External-Sampling MCCFR**   ES chance sample,  对手 按其策略sample,  玩家自己全遍历；  这个最方便，$z$的采样概率正好就是$\pi_{-i}$



### 2 Background

#### 2.1 Strategies and Equilibria

a player's **best response** :  strategy that maximizes their expected payoff assuming all other players play according to $\sigma .$   固定其他人的策略, 自己走最优. 

**best-response value** for player $i$ is the value of that strategy, $$b_{i}\left(\sigma_{-i}\right)=\max _{\sigma_{i}^{\prime} \in \Sigma_{i}} u_{i}\left(\sigma_{i}^{\prime}, \sigma_{-i}\right) $$. 

$\epsilon$ -Nash equilibrium is an **approximation** of a Nash equilibrium

$$
\forall i \in N \quad u_{i}(\sigma)+\epsilon \geq \max _{\sigma_{i}^{\prime} \in \Sigma_{i}} u_{i}\left(\sigma_{i}^{\prime}, \sigma_{-i}\right)
$$

If a game is **two-player and zero-sum**, we can use **exploitability** as a **metric** for determining how close $\sigma$ is to an equilibrium, $\epsilon_{\sigma}=b_{1}\left(\sigma_{2}\right)+b_{2}\left(\sigma_{1}\right)$ 



#### 2.2 Counterfactual Regret Minimization

**Regret**  is an **online learning** concept that has triggered a family of powerful learning algorithms. To define this concept, first consider repeatedly playing an extensive game.

Let $\sigma_{i}^{t}$ be the strategy used by player $i$ on round $t .$ The **average overall regret** of player $i$ at time $T$ is:

$$
R_{i}^{T}=\frac{1}{T} \max _{\sigma_{i}^{*} \in \Sigma_{i}} \sum_{t=1}^{T}\left(u_{i}\left(\sigma_{i}^{*}, \sigma_{-i}^{t}\right)-u_{i}\left(\sigma^{t}\right)\right)
$$

define $$\bar{\sigma}_{i}^{t}$$ to be the **average strategy** for player $i$ from time 1 to $T .$ In particular, for each information set $$I \in \mathcal{I}_{i},$$ for each $$a \in A(I),$$ define:

$$
\bar{\sigma}_{i}^{t}(a \mid I)=\frac{\sum_{t=1}^{T} \pi_{i}^{\sigma^{t}}(I) \sigma^{t}(a \mid I)}{\sum_{t=1}^{T} \pi_{i}^{\sigma}(I)}
$$

There is a **well-known connection** between **regret**, **average strategies**, and **Nash equilibria**.

**Theorem 1**   In a **zero-sum** game, if $R_{i \in\{1,2\}}^{T} \leq \epsilon$, then $\bar{\sigma}^{T}$ is a $2 \epsilon$ equilibrium.

如果可以使得$$R_{i}^{T}$$ 随着$t$增大到无穷大而趋向零, 那么该算法就是 **遗憾最小化算法 Regret Minimization.** 

在self-play中, Regret Minimization可以作为一种计算 近似纳什均衡的技术。此外，算法对average overall regret的约束, 也约束了近似的收敛速度。

CFR的基本思想是，**average overall regret** can be **bounded** by the **sum** of **positive** per-information-set **immediate counterfactual regret** .  总体的regret 可以被 所有信息集的 positive immediate counterfactual regret 的sum 所约束.

Let $Z_{I}$ be the subset of all terminal histories where a prefix of the history is in the set $I$; $Z_{I}$ 是是所有结局中与从信息集$I$ 出发的.
for $z \in Z_{I}$ let $z[I]$ be that prefix.  
since we are restricting ourselves to perfect recall games $z[I]$ is unique.  **对perfect recall的game, $z$ 对应的 $h=z[I]$ 是唯一的**

**counterfactual value** $v_{i}(\sigma, I)$ :

$$
v_{i}(\sigma, I)=\sum_{z \in Z_{I}} \pi_{-i}^{\sigma}(z[I]) \pi^{\sigma}(z[I], z) u_{i}(z)  \tag{4}
$$

这里， 使用的符号 不是 从某个$h$再到$z$.   表述为 $z[I]$ 到 $z$ ,更简单一些。  用$h$表示为 

$$v_{i}(\sigma, h)=\sum_{z \in Z, h \sqsubset z} \pi_{-i}^{\sigma}(h) \pi^{\sigma}(h, z) u_{i}(z)$$



The **immediate counterfactual regret** is then $$R_{i, \mathrm{imm}}^{T}(I)=\max _{a \in A(I)} R_{i, \mathrm{imm}}^{T}(I, a),$$ where

$$
R_{i, \mathrm{imm}}^{T}(I, a)=\frac{1}{T} \sum_{t=1}^{T}\left(v_{i}\left(\sigma_{(I \rightarrow a)}^{t}, I\right)-v_{i}\left(\sigma^{t}, I\right)\right) \tag{5}
$$



**Theorem 2**  $$\quad R_{i}^{T} \leq \sum_{I \in \mathcal{I}_{i}} R_{i, \text { imm }}^{T,+}(I)$$

使用 **regret-matching** 算法,可以使得 **positive per-information set immediate counterfactual regrets** (上式的右边)慢慢逼近0 , 于是使得 average overall regret(上式左边) 趋向0. 

这个结果, 提出了一种 通过self-play 来计算 均衡策略的算法, 被称为  vanilla CFR.   
想法是使用公式4来遍历game tree,计算 counterfactual values. 给定一个策略,  
这些cf values 以及 公式5 计算各个玩家在每个infoset上的regret.   
这些 regret 累计起来, 通过 **regret-matching** 公式来计算下一轮的策略.  
因为两个玩家都是regret minimizing, 通过 Theorem 1 可知计算 平均策略 $\bar{\sigma}^{t}$ , 得到了近似均衡策略. 

CFR 只需在每个信息集存储值, 所以 空间需求是 $O(\|\mathcal{I}\|) .$   
然而，vanilla CFR 需要在每次迭代时对博弈树进行完整的遍历，这禁止它在许多大型游戏中使用。Zinkevich采取了一些措施来缓解这个问题，即chance-sampled 变体, 来解poker类的问题.








### 3 Monte Carlo CFR

MCCFR的关键是, 避免在每次迭代时遍历整个博弈树，同时让immediate CFR 的预期 保持不变。
我们希望每次迭代时限制终端历史的范围。 

> 直接画出树，算出q比较直观，下面是通用的表述。 

令 $$\mathcal{Q}=\left\{Q_{1}, \ldots, Q_{r}\right\}$$, 是$Z$ 的子集的集合,  覆盖 $Z$.  将把这些子集中的一个称为 **块block**。  一个$z$只在一个block中。
在每次迭代时，我们将采样出一个块，并只考虑该块中的那些结局(这时开始全遍历)。  
令 $q_{j}>0$ ,  在当前迭代中选择 block $Q_{j}$ 的概率 .     $\sum_{j=1}^{r} q_{j}=1$ ， 这个是选择某个block的概率，加起来要等于1.

Let $$q(z)=\sum_{j: z \in Q_{j}} q_{j}$$ ,  i.e., $q(z)$ is the probability of considering terminal history $z$ on the current iteration.    z在本次迭代中出现的几率
这个q(z)是z 被sample到的几率。  一个$z$ 一般在某个$Q_j$中。 

The **sampled counterfactual value** when updating block $j$ is:  采样的虚拟收益,  有点类似于**重要性采样**

$$
\tilde{v}_{i}(\sigma, I | j)=\sum_{z \in Q_{j} \cap Z_{I}} \frac{1}{q(z)} u_{i}(z) \pi_{-i}^{\sigma}(z[I]) \pi^{\sigma}(z[I], z)  \tag{6}
$$

跟之前的一样， 只是多除了q； 每个block中的所有z 的 r/q 的sum 。 

关键是这里的 q 怎么算，才能无偏。

从某个信息集$I$ 出发，所有的结局 $Z_I$ ,  其中的每个z，只要属于block j ， 就会更新 block j ； 

通过采样 $\mathcal{Q}$  定义了 sample-based CFR 算法。不是完整的游戏树遍历，而是采样其中一个block，然后只检查该block中的终端历史。



假设我们选择 $Q=\{Z\}$ , 即，一个块包含所有的终端历史, 则 $q_{1}=1 .$ 在这种情况下，采样的CFR等于CFR，**vanilla CFR**.     
假设我们选择每个block中，完全按照chance节点的概率来划分（如骰子，就弄6个block， 如果投掷两次，则36个block）：$q_j$ 是chance节点 的序列的采样几率的积。就有了Zinkevich的**chance-sampled  CFR**。  

Sampled counterfactual value被设计成与预期的counterfactual value相匹配。我们在这里证明了这一点，然后在下一节中用这个事实证明算法的平均总regret的概率约束。



##### Lemma 1

$$
E_{j \sim q_{j}}[\tilde{v_{i}}(\sigma, I | j)]=v_{i}(\sigma, I)
$$

Proof:

$$
\begin{aligned}
E_{j \sim q_{j}}\left[\tilde{v}_{i}(\sigma, I | j)\right] &=\sum_{j} q_{j} \tilde{v}_{i}(\sigma, I | j)=\sum_{j} \sum_{z \in Q_{j} \cap Z_{i}} \frac{q_{j}}{q(z)} \pi_{-i}^{\sigma}(z[I]) \pi^{\sigma}(z[I], z) u_{i}(z) \\
&=\sum_{z \in Z_{I}} \frac{\sum_{j: z \in Q_{j}} q_{j}}{q(z)} \pi_{-i}^{\sigma}(z[I]) \pi^{\sigma}(z[I], z) u_{i}(z) \\
&=\sum_{z \in Z_{I}} \pi_{-i}^{\sigma}(z[I]) \pi^{\sigma}(z[I], z) u_{i}(z)=v_{i}(\sigma, I)
\end{aligned}
$$

得到下面的MCCFR算法。  
sample a block and for each information set that contains a prefix of a terminal history in the block ，  计算出每个动作的 **sampled immediate counterfactual regrets**，$$\tilde{r}(I, a)=\tilde{v}_{i}\left(\sigma_{(I \rightarrow a)}^{t}, I\right)-\tilde{v}_{i}\left(\sigma^{t}, I\right)$$ . player's strategy on the next iteration applies the regret-matching algorithm to the accumulated regrets

现在我们介绍一下这个家族中的两个具体成员，详细介绍一下如何有效地更新遗憾值的具体方法



<img src="/img/2020-04-21-MCCFR.assets/image-20200519235016760.png" alt="image-20200519235016760" style="zoom:50%;" />

图来自 Targeted CFR

看图很容易理解.



##### Outcome-Sampling MCCFR

each block contains a single terminal history, i.e., $\forall Q \in \mathcal{Q},\vert Q \vert =1$ .  每个block只含一个结局.  每次挑一个结局, 但怎么挑路径走到这个结局是有讲究的.

如果z在采样路径上则 u/q ，如果某个在不在，则u=0



这段描述比较清楚， 都遵从当前策略， 但有可能缺乏探索性。

In Outcome Sampling, a single action is sampled at every information set. 对每个信息集sample一个action.   

- At an information set where **chance** acts, we sample according to the **fixed chance distribution**. 
- At an information set where the **opponent** acts, we sample according to the **opponent’s current strategy**. 
- At an information set where the **target player** acts, we sample *approximately* according to the **target player’s current strategy**. 

The complication is that we require some “**exploration**” to ensure that the reachability requirement is satisfied. For example, with some small probability ε we may sample an action according to the uniform distribution.

这里需要探索， 所以对target player的strategy是个近似。 

A single iteration of Outcome Sampling involves following a single trajectory from the root of the game tree to a terminal history $z$. 



在每次迭代中，我们对一个终端历史进行采样，并且**只更新沿该历史的每个信息集**。采样概率 $q_{j}$ 是在终端历史上的分布。我们使用sampling profile  $\sigma^{\prime}$来指定这个分布,  $q(z)=\pi^{\sigma^{\prime}}(z) $  .  注意，任何抽样策略都会在block概率$q(z)$ 上导致特定分布。As long as $\sigma_{i}^{\prime}(a \vert I)>\epsilon$, then there exists a $\delta>0$ such that $q(z)>\delta$, 保证公式6是 不会除以0， well-defined.

该算法的工作原理是通过使用策略$\sigma^{\prime}$来采样$z$，存储$\pi^{\sigma^{\prime}}(z)$.  然后向前遍历单个历史（计算每个玩家的到达$h$ 的概率， playing to reach each prefix of the history, $\pi_{i}^{\sigma}(h)$）和向后遍历（计算每个玩家的概率 $h \to z$，playing the remaining actions of the history, $\pi_{i}^{\sigma}(h, z)$）。在后向遍历过程中，计算出每个**访问过的信息集**上的sampled counterfactual regrets（并加入到total regret中）。   

更新 regret ， 如果$I$ 上选a 会抵达本次sample的$z$ ， 跟之前一样, 只不过放大/q； 如果$I$ 上选a 不经过本次sample的$z$ ， 收益是0 
$$
\tilde{r}(I, a)=\left\{\begin{array}{cl}
w_{I} \cdot(1-\sigma(a | z[I])) & \text { if }(z[I] a) \sqsubseteq z \\
-w_{I} \cdot \sigma(a | z[I]) & \text { otherwise }
\end{array}, \text { where } w_{I}=\frac{u_{i}(z) \pi_{-i}^{\sigma}(z) \pi_{i}^{\sigma}(z[I] a, z)}{\pi^{\sigma^{\prime}}(z)}\right. \tag{10}
$$





outcome-sampling MCCFR的一个优点是，如果我们的终端历史是根据对手的策略进行采样的，$\sigma_{-i}^{\prime}=\sigma_{-i}$ ,  那么更新就不再需要$\sigma_{-i}$的显式的知识，因为它抵消了 $\sigma_{-i}^{\prime}$ .  所以 $w_{I}$ 变成 $u_{i}(z) \pi_{i}^{\sigma}(z[I], z) / \pi_{i}^{\sigma^{\prime}}(z)$  . 因此，我们可以使用outcomesampling MCCFR进行在线 **online** regret minimizatio。我们必须选择我们自己的行动，以便使 $\sigma_{i}^{\prime} \approx \sigma_{i}^{t}$，但要保证**探索性exploration** $q_{j} \geq \delta>0$ 。通过平衡探索引起的遗憾和$\delta$ 的遗憾, （参见第4节），我们可以对平均总遗憾进行约束，只要事先知道游戏次数T。这有效地模仿了regret minimization in normal-form games[9]。建议采用方程10的另一种形式来实现 [10] . 



##### External-Sampling MCCFR

只sample外部因素.  即对手以及chance节点是sample，当前player是遍历所有action；  相当于, 面对一次随机的外部的纯策略. 
sample only the actions of the opponent and chance (those choices external to the player).  

External Sampling is like Outcome Sampling at information sets where **chance** acts or where the **opponent** acts; i.e., we **sample** a single action according to the current strategy profile. External Sampling differs at information sets where the **target player** acts. At these information sets we **evaluate all actions**. Since we evaluate all of the target player actions we **meet the reachability requirement**.

对每个对手跟chance的纯策略都有一个对应的block $$Q_{\tau} \in \mathcal{Q}$$  , 即对应每个**deterministic** mapping $$\tau$$ from $$I \in \mathcal{I}_{c} \cup   \tilde{\mathcal{I}}_{N \backslash\{i\}}$$ to $$\vec{A}(I) .$$    
block probabilities 按照  $$f_{c}$$ and $$\sigma_{-i}$$ 的分布来分配的. 所以 $$q_{\tau}=\prod_{I \in \mathcal{I}_{c}} f_{c}(\tau(I) | I) \prod_{I \in \mathcal{I}_{N} \backslash\{i\}} \sigma_{-i}(\tau(I) | I)$$ .  
那么这个块$$Q_{\tau}$$包含了所有符合 $$\tau$$ 的结局 $$ z$$  
就是说, if $$h a$$ is a prefix of $$z$$ with $$h \in I$$ for some $$I \in \mathcal{I}_{-i}$$ then $$\tau(I)=a$$. 

在实践中，我们实际上不会对$\tau$ 进行抽样，而是只根据需要对构成$\tau$ 的单个动作进行抽样。关键是，这些块的概率使得 $q(z)=\pi_{-i}^{\sigma}(z)$ . 该算法对$i \in N$ 进行迭代，每次做一次深度优先的后序遍历post-order depth-first traversa，在每一个历史h处采样操作，其中$P(h) \neq i$（存储这些选择，因此在同一信息集中的所有h处都会采样相同的操作）。由于完美recall，在这个遍历过程中，它永远不能从同一信息集中访问一个以上的历史。对于每一个这样的访问过的信息集， sampled counterfactual regrets 都会计算出（并加到总遗憾值中）。


$$
\tilde{r}(I, a)=(1-\sigma(a | I)) \sum_{z \in Q \cap Z_{i}} u_{i}(z) \pi_{i}^{\sigma}(z[I] a, z)
$$

请注意，在遍历过程中，通过维护 weighted sum of the utilities of all terminal histories rooted at the current history，可以很容易地计算出总和。







### 4 Theoretical Analysis



We now present regret bounds for members of the MCCFR family, starting with an improved bound for vanilla CFR that depends more explicitly on the exact structure of the extensive game. Let $$\vec{a}_{i}$$ be a subsequence of a history such that it contains only player $$i$$ 's actions in that history, and let $$\vec{A}_{i}$$ be the set of all such player $$i$$ action subsequences. Let $$\mathcal{I}_{i}\left(\vec{a}_{i}\right)$$ be the set of all information sets where player $$i$$ 's action sequence up to that information set is $$\vec{a}_{i}$$. Define the $$M$$ -value for player $$i$$ of the game to be $$M_{i}=\sum_{\vec{a}_{i} \in \vec{A}_{i}} \sqrt{\left \vert \mathcal{I}_{i}(\vec{a})\right \vert } .$$ Note that $$\sqrt{\left \vert \mathcal{I}_{i}\right \vert } \leq M_{i} \leq\left \vert \mathcal{I}_{i}\right \vert $$ with both sides of this bound
being realized by some game. We can strengthen vanilla CFR's regret bound using this constant. which also appears in the bounds for the MCCFR variants.




##### Theorem 3

When using vanilla CFR for player i, $R_{i}^{T} \leq \Delta_{u, i} M_{i} \sqrt{\left \vert A_{i}\right \vert } / \sqrt{T}$

We now turn our attention to the MCCFR family of algorithms, for which we can provide probabilistic regret bounds. We begin with the most exciting result: showing that **external-sampling requires only a constant factor more iterations than vanilla CFR** (where the constant depends on the desired confidence in the bound)



##### Theorem 4 

For any $p \in(0,1]$, when using external-sampling MCCFR, with probability at least $1-p,$ average overall regret is bounded by, $R_{i}^{T} \leq\left(1+\frac{\sqrt{2}}{\sqrt{p}}\right) \Delta_{u, i} M_{i} \sqrt{\left \vert A_{i}\right \vert } / \sqrt{T}$

Although requiring the same order of iterations, note that external-sampling need only traverse a fraction of the tree on each iteration. For balanced games where players make roughly equal numbers of decisions, the iteration cost of external-sampling is $O(\sqrt{ \vert H \vert }),$ while vanilla CFR is $O( \vert H \vert )$ meaning **external-sampling MCCFR requires asymptotically less time to compute an approximate equilibrium than vanilla CFR** (and consequently chance-sampling CFR, which is identical to vanilla CFR in the absence of chance nodes.



##### Theorem 5

For any $p \in(0,1]$, when using outcome-sampling $M C C F R$ where $\forall z \in Z$ either $\pi_{-i}^{\sigma}(z)=0$ or $q(z) \geq \delta>0$ at every timestep, with probability $1-p,$ average overall regret is bounded by $R_{i}^{T} \leq\left(1+\frac{\sqrt{2}}{\sqrt{p}}\right)\left(\frac{1}{\delta}\right) \Delta_{u, i} M_{i} \sqrt{\left \vert A_{i}\right \vert } / \sqrt{T}$








## Reference

Monte Carlo Sampling for Regret Minimization in Extensive Games

Targeted CFR











