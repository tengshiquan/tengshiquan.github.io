---
layout:     post
title:      TRPO PPO
subtitle:   
date:       2020-02-22 12:00:00
author:     "tengshiquan"
header-img: "img/about-bg.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning
    - PPO
    - TRPO
---



# TRPO & PPO

#### Issue of Importance Sampling

PG å¼•å…¥ IS å, ä»on-policy å˜æˆ off-policy.  æœŸæœ›æ˜¯ä¸€æ ·çš„. ä½†æ–¹å·®ä¸ä¸€æ ·. 
$$
VAR[X] = E\left[X^{2}\right]-(E[X])^{2}
$$

$$
E_{x \sim p}[f(x)]=E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]
$$

$$
\operatorname{Var}_{x \sim p}[f(x)] \quad \operatorname{Var}_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]
$$

$$
\operatorname{Var}_{x \sim p}[f(x)]=E_{x \sim p}\left[f(x)^{2}\right]-\left(E_{x \sim p}[f(x)]\right)^{2}  \tag{1}
$$


$$
\begin{aligned}
\operatorname{Var}_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]  &=E_{x \sim q}\left[\left(f(x) \frac{p(x)}{q(x)}\right)^{2}\right]-\left(E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]\right)^{2}\\
&=E_{x \sim p}\left[f(x)^{2} \frac{p(x)}{q(x)} \right] -\left(E_{x \sim p}[f(x)]\right)^{2} 
\end{aligned}  \tag{2}
$$


æ˜¾ç„¶, å…¬å¼2ä¸å…¬å¼1, å·®åˆ«å°±åœ¨äºå‡å·å‰é¢çš„ç¬¬ä¸€é¡¹ .   å…¬å¼2å¤šäº†ä¸€ä¸ª p/q . å¦‚æœp,qçš„åˆ†å¸ƒç›¸å·®å¾ˆå¤§, åˆ™è¯¥æ¯”å€¼å˜åŒ–ä¼šå¾ˆå¤§ , é€ æˆæ–¹å·®å˜å¤§. 



<img src="/img/2020-02-22-TRPO-PPO.assets/image-20200330020120268.png" alt="image-20200330020120268" style="zoom:50%;" />

- å¦‚å›¾, æ€»ä½“ä¸Š, æŒ‰ç…§pæ¥é‡‡æ ·, $E_{x \sim p}[f(x)]$æ˜¯è´Ÿæ•°.   
- å¦‚æœæŒ‰ç…§qæ¥é‡‡æ ·, å› ä¸ºqåœ¨å³è¾¹å‡ ç‡é«˜, åªé‡‡æ ·åˆ° å³è¾¹çš„å‡ ä¸ªç»¿ç‚¹, åˆ™æŒ‰ç…§ISå…¬å¼ç®— $E_{x \sim p}[f(x)]$ å¯èƒ½å°±æ˜¯æ­£æ•°äº†. 
- å¦‚æœç¢°å·§é‡‡æ ·åˆ°å·¦è¾¹çš„é‚£ä¸ªç»¿è‰²ç‚¹, å› ä¸ºp/qè¿™ä¸ªæ—¶å€™å€¼è¶…çº§å¤§, æœ‰å¯èƒ½æŠŠ$E_{x \sim p}[f(x)]$ ç»™çº æ­£å›æ¥. 
- æ‰€ä»¥å¿…é¡»sampleè¶³å¤Ÿå¤šæ¬¡..





#### On-policy â†’ Off-policy

ä¸‹é¢ç”¨ $\theta'$ é‡‡æ ·, åšdemostration

Gradient for update

$$
\nabla f(x)=f(x) \nabla \log f(x)
$$

$$
\begin{aligned}
\text{Gradient for update} 
&=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta}}\left[A^{\theta}\left(s_{t}, a_{t}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)\right] \\
&=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{P_{\theta}\left(s_{t}, a_{t}\right)}{P_{\theta^{\prime}}\left(s_{t}, a_{t}\right)}  A^{\theta \color{red}{^{\prime}} }\left(s_{t}, a_{t}\right)   \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)\right] \\
&=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} \frac{p_{\theta}\left(s_{t}\right)}{p_{\theta^{\prime}}\left(s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)\right]
\end{aligned}
$$

ç¬¬äºŒè¡Œçš„ ä»$A^{\theta}$ æ”¹ä¸ºç”¨ $A^{\theta'}$ è¿‘ä¼¼, å‡è®¾ä¸¤ä¸ªå·®ä¸å¤š.   ç¬¬ä¸‰è¡Œå‡è®¾ $p_\theta(s_t)$ ä¸ $p_{\theta'}(s_t)$ åˆ†å¸ƒå·®ä¸å¤š, åˆ†å­åˆ†æ¯æ¶ˆæ‰. 

ä¸‹é¢ç”±ä¸Šé¢çš„è¿‘ä¼¼gradientæ–¹å¼, å»åæ¨ä¸€ä¸ªè¿‘ä¼¼çš„å…³äº$\theta$ ç›®æ ‡å‡½æ•°. 

$$
J^{\theta^{\prime}}(\theta)=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right)\right]
$$


#### Add Constraint

ä¸Šé¢çš„ä¸¤ä¸ªè¿‘ä¼¼çš„å‰æ,å°±æ˜¯ $\theta$ ä¸ $\theta'$ ä¸èƒ½å·®å¤ªå¤š.  æ‰€ä»¥åŠ ä¸€ä¸ªé™åˆ¶é¡¹, ç±»ä¼¼æ­£åˆ™é¡¹, é™åˆ¶learnå‡ºæ¥çš„$\theta$  ä¸ $\theta'$ä¸èƒ½è¿‡äºä¸ä¸€æ ·. (å…¶å®æ˜¯æŒ‡policyä¸èƒ½å·®å¼‚è¿‡å¤§)

ğœƒ cannot be very different from ğœƒâ€² ; Constraint on behavior not parameters.   
å¦å¤–, æ³¨æ„,å…¬å¼ä¸­ è¿™é‡ŒKL divergence ä¸æ˜¯æŒ‡å‚æ•° $\theta$ ä¸ $\theta'$ distributionçš„è·ç¦», è€Œæ˜¯ä¸¤ä¸ªmodeläº§å‡ºçš„action, å³policyçš„æ¦‚ç‡è·ç¦». 





##### **Proximal Policy Optimization (PPO)**

$$
\begin{array}{c}
J_{P P O}^{\theta^{\prime}}(\theta)=J^{\theta^{\prime}}(\theta)-\beta K L\left(\theta, \theta^{\prime}\right) \\
J^{\theta^{\prime}}(\theta)=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right)\right]
\end{array}
$$

PPOæŠŠé™åˆ¶é¡¹ä½œä¸ºäº†ä¸€ä¸ªæ­£åˆ™é¡¹æ”¾å…¥ç›®æ ‡Jå‡½æ•°ä¸­. 

##### **TRPO (Trust Region Policy Optimization)**

$$
\begin{aligned}
J_{T R P O}^{\theta^{\prime}}(\theta)=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right)\right] & \\
& K L\left(\theta, \theta^{\prime}\right)<\delta
\end{aligned}
$$

TRPOæ˜¯åœ¨åšä¸€ä¸ª å¸¦é™åˆ¶æ¡ä»¶çš„ æœ€ä¼˜åŒ–é—®é¢˜ , å¾ˆéš¾ç®—.  PPOä¸TRPO performanceå·®ä¸å¤š.



#### PPO algorithm

- Initial policy parameters $\theta^{0}$
- In each iteration
  - Using $\theta^{k}$ to interact with the environment to collect $$\left\{s_{t}, a_{t}\right\}$$ and compute advantage $A^{\theta^{k}}\left(s_{t}, a_{t}\right)$ 
  - Find $\theta$ optimizing $J_{P P O}(\theta)$
    $J_{P P O}^{\theta^{k}}(\theta)=J^{\theta^{k}}(\theta)-\beta K L\left(\theta, \theta^{k}\right) \begin{array}{l}\text { Update parameters } \\ \text { several times }\end{array}$

å› ä¸ºè¿™é‡Œæ˜¯off-policy , æ‰€ä»¥å¯ä»¥ update å‚æ•°å¤šæ¬¡è€Œä¸ç”¨ç«‹åˆ»å»é‡æ–°é‡‡æ ·. 



##### Adaptive KL Penalty

è¿™ä¸ª KL æƒ©ç½šé¡¹ çš„è¶…å‚å¯ä»¥åŠ¨æ€è°ƒæ•´. 

-  If $K L\left(\theta, \theta^{k}\right)>K L_{\max },$ increase $\beta$
-  If $K L\left(\theta, \theta^{k}\right)<K L_{\text {min }},$ decrease $\beta$ 
  



##### **PPO2 algorithm** 

å› ä¸ºç®—KLè¿˜æ˜¯éº»çƒ¦.  
$$
\begin{aligned}
J_\text{PPO2}^{\theta^k}(\theta) \approx \sum_{\left(s_{t}, \boldsymbol{a}_{t}\right)} \min \Bigg(\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{k}}\left(a_{t} | s_{t}\right)} A^{\theta^{k}}\left(s_{t}, a_{t}\right) ,   
& \operatorname{clip}\left(\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{k}}\left(a_{t} | s_{t}\right)}, 1-\varepsilon, 1+\varepsilon\right) A^{\theta^{k}\left(s_{t}, a_{t}\right)}\Bigg)
\end{aligned}
$$
<img src="/img/2020-02-22-TRPO-PPO.assets/image-20200330042218479.png" alt="image-20200330042218479" style="zoom:50%;" /> æ¨ªè½´å°±æ˜¯ä¸¤ä¸ªpçš„æ¯”å€¼. 

<img src="/img/2020-02-22-TRPO-PPO.assets/image-20200330042255765.png" alt="image-20200330042255765" style="zoom:50%;" />

æ‰€ä»¥çœ‹å›¾åƒ, æœ€åçš„æ•ˆæœ, å¯ä»¥é˜²æ­¢$p_\theta$  ä¸ $p_{\theta'}$ å·®è·è¿‡å¤§.  çœ‹A>0çš„æƒ…å†µ, ä¼šå¢åŠ  $\frac{p_\theta}{p_{\theta'}}$Â , å³è¯¥actionå‡ºç°çš„å‡ ç‡, ä½†ä¸å¸Œæœ›ä¸¤ä¸ªå·®è·è¿‡å¤§, ä¸å¯ä»¥è¶…è¿‡$1+\varepsilon$, æ‰€ä»¥ clip .  



##### Experimental Results

https://arxiv.org/abs/1707.06347

![image-20200330043234698](/img/2020-02-22-TRPO-PPO.assets/image-20200330043234698.png)





ç”±äºé‡è¦æ€§é‡‡æ ·çš„å…³ç³»æˆ‘ä»¬å¸Œæœ›æ¯æ¬¡æ›´æ–°çš„æ—¶å€™ç­–ç•¥åˆ†å¸ƒä¹‹é—´å·®è·å¹¶ä¸æ˜¯å¾ˆå¤§ï¼Œè¿™å®é™…ä¸Šæ˜¯ä¸€ç§çº¦æŸï¼Œå³æˆ‘ä»¬å¸Œæœ›èƒ½æ¯æ¬¡æ›´æ–°çš„æ—¶å€™ä¸å¤§å¹…åº¦åœ°æ”¹å˜åˆ†å¸ƒçš„å½¢æ€ï¼ŒåŸºäºè¿™ç§è€ƒè™‘openaiçš„å‰è¾ˆä»¬æå‡ºäº†TRPOç®—æ³•ï¼Œä½†æ˜¯TRPOç®—æ³•ä¼šæœ‰ä¸€äº›ç¼ºé™·ï¼Œä»–æ‹¿äºŒæ¬¡å‡½æ•°å»è¿‘ä¼¼çº¦æŸæ¡ä»¶ï¼Œæ‹¿ä¸€æ¬¡å‡½æ•°è¿‘ä¼¼å¾…ä¼˜åŒ–çš„æŸå¤±å‡½æ•°ï¼Œè¿™ç§è¿‘ä¼¼ä¼šé€ æˆæ”¶æ•›ä¸Šçš„å›°éš¾ï¼Œäºæ˜¯ä¾¿æœ‰äº†ç¬¬äºŒæ¬¡smartçš„æ”¹è¿›ï¼Œå¾—åˆ°PPOç³»åˆ—çš„ç®—æ³•

ç­–ç•¥æ¢¯åº¦çš„ç¡¬ä¼¤å°±åœ¨äºæ›´æ–°æ­¥é•¿$\alpha$,å½“æ­¥é•¿é€‰çš„ä¸åˆé€‚çš„æ—¶å€™æ›´æ–°çš„å‚æ•°ä¼šæ›´å·®ï¼Œå› æ­¤å¾ˆå®¹æ˜“å¯¼è‡´è¶Šå­¦è¶Šå·®ï¼Œæœ€åå´©æºƒï¼Œé‚£ä»€ä¹ˆæ ·çš„æ­¥é•¿å«åšåˆé€‚çš„æ­¥é•¿å‘¢ï¼Œè¯•æƒ³æˆ‘ä»¬å¦‚æœèƒ½æ‰¾åˆ°ä¸€ç§æ­¥é•¿ï¼Œä½¿ä»–æ¯æ¬¡æ›´æ–°æ—¶éƒ½èƒ½ä¿è¯å›æŠ¥å‡½æ•°**å•è°ƒé€’å¢**ï¼Œè¿™æ ·çš„æ­¥é•¿å°±æ˜¯å¥½æ­¥é•¿ã€‚TRPOçš„æ ¸å¿ƒå°±æ˜¯è§£å†³è¿™ä¸ªé—®é¢˜ã€‚









çº ç»“çš„ç‚¹, æ˜¯å­¦ä¹ ç‡  ;    è¿˜æœ‰é—®é¢˜èƒ½ä¸èƒ½è¢«å‡½æ•°è¡¨è¾¾;  å¦‚æœå¯ä»¥çš„è¯, æ„Ÿè§‰trpoè¿˜æ˜¯æœ‰ç‚¹åˆç†æ€§çš„

æ‹Ÿåˆå‡½æ•°æœ¬èº«çš„å‡†ç¡®æ€§,  é‡‡æ ·,  ç­‰ç­‰å„ç§é—®é¢˜



è¦è¿…é€Ÿç»„æˆå¸¸è§æ¨¡å¼.. 



æ–°ç­–ç•¥ , å¯¹å¯èƒ½å‡ºç°çš„çŠ¶æ€, ä»¥åŠå¯èƒ½å‡ºç°çš„é£é™©, è¦èƒ½é¢„åˆ¤, å¹¶åŠ å…¥åˆ° è€ƒè™‘ä¸­.. 

é€‰æ‹©action, ç”±ç²—åˆ°ç»† ...



è§†é‡ç‹­çª„é—®é¢˜, åªèƒ½é‡‡æ · ;  å¯¹å›¾åƒ, è‡³å°‘èƒ½  CNN












