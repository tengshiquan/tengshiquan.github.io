---
layout:     post
title:      CFR
subtitle:   Regret Minimization in Games with Incomplete Information
date:       2020-04-14 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-dice.jpg"
catalog: true
tags:
    - AI
    - CFR
    - Imperfect Information
    - Alberta
    - Texas
    - Game Theory

---



## Regret Minimization in Games with Incomplete Information

2008   **Zinkevich** from Alberta  ,  提出 CFR 算法 ,  重要论文



### Abstract

描述了一种基于遗憾最小化 regret minimization 的大型博弈求解新技术。特别是，我们引入了虚拟遗憾**counterfactual regret**的概念，它利用了博弈中不完全信息的程度。我们展示了虚拟遗憾最小化如何使总体遗憾最小化，因此在自博弈中可以用来计算纳什均衡。我们在扑克领域演示了这一技术，表明我们可以解决极限德州扑克的抽象状态多达$10^{12}$个状态，比以前的方法大两个数量级。



### 1 Introduction

**扩展式博弈Extensive games** 是在有其他决策者在场的情况下进行顺序决策的一种自然模型，特别是在信息不完全的情况下，决策者对博弈状态有不同的信息。与其他模型（如MDPs和POMDPs）一样，它的有用性取决于能解决的模型的规模。对于非常大规模的扩展博弈的求解技术最近受到了相当大的关注，扑克牌成为了衡量性能的常用指标。扑克游戏可以很自然地被建模为扩展博弈，即使限制了规模，如双人limit押注德州扑克等，也是不切实际的大规模游戏，达$10^{18}$个游戏状态。

解决扩展博弈，传统上将plan表示为**线性编程linear programming** 解决[1]。这种表示方式在博弈状态的数量上是线性的，而不是指数型的，但要处理扑克牌那样大小的博弈，仍然需要相当多的额外技术。抽象技术 **Abstraction**，包括手工[2]和自动[3]，通常被用来将游戏从$10^{18}$个游戏状态数减少到一个可操作的游戏状态数（例如$10^7$个），同时仍能产生强大的扑克程序。此外，将游戏分成多个子游戏，每个子游戏独立或实时求解，也被研究过[2，4]。解决更大的抽象可以得到较好的近似纳什均衡，这使得求解规模较大博弈的技术重点研究该领域。最近，有人提出了迭代技术作为传统的线性编程方法的替代方法。这些技术已经被证明能够找到多达$10^{10}$个博弈状态的近似解[5，6，7]，导致了近似解在过去四年来扑克程序中的首次重大改进。

在本文中，我们描述了一种新技术，用于寻找大型扩展博弈的近似解。该技术是基于最小化遗憾，使用一个新的概念，即**虚拟遗憾counterfactual regret**。我们表明，虚拟遗憾最小化可以使总体遗憾最小化，因此可以用来计算纳什均衡。然后，我们提出了一种在扑克游戏中最小化虚拟遗憾的算法。我们用该算法求解多达$10^{12}$个游戏状态的扑克抽象，比以前的方法大了两个数量级。我们还表明，这直接转化成了扑克游戏程序强度的提高。我们首先对扩展博弈进行了正式的描述，然后概述了遗憾最小化及其与纳什均衡的关系。



### 2 Extensive Games, Nash Equilibria, and Regret

Extensive Games提供了一个通用而紧凑的多代理交互模型，它明确地表示了这些交互的顺序性。在提出正式的定义之前，我们首先给出一些直觉。Extensive Games的核心是一个博弈树，就像完美信息博弈（如国际象棋或围棋）一样。每一个非结束状态都有一个相关的玩家选择行动，而每一个结束状态都有每个玩家的相关回报。关键的区别在于**信息集information sets**的附加约束，信息集是当前玩家无法区分的游戏状态集，因此必须为所有这些状态选择具有相同分布的行动。例如，在扑克游戏中，第一个行动的玩家不知道其他玩家被发的是哪张牌，所以在发牌后紧接着的所有游戏状态中，第一个玩家持有相同的牌都在同一个信息集中。我们现在描述一下形式上的模型，以及后面会用到的符号。 下面的i,一般都指玩家i



##### Definition 1 信息不完全扩展博弈 

一个信息不完全的有限扩展博弈有以下几个部分:

1. 玩家的有限集 ,  $N$ 
2. **action sequences** 的有限集 ,  $H$  , 即所有可能的 **行为历史 histories** of actions;   
   显然, 空的序列，以及$H$中的某个序列的**前缀prefix序列**也在$H$中。    
   注意, 这里的序列不是经历过的状态的序列, 而是采取过的行为的序列.  博弈论似乎不强调state, 强调action. 认为, 可以由行为序列得到 状态序列.  如果是随机性的, 则把随机数看成一个行为, 也的确可以做到. 
3. $Z \subseteq H$ 是包含终点terminal的历史（显然不会是其他序列的前缀）。
4. $A(h)=\{a:(h, a) \in H\}$ 是在某个给定 $h\in H$之后 所能采取的行为集合。 
5. ***player function*** :  函数$P$, 决定某个非终结时刻$ h \in H \backslash Z$ , 哪个玩家行动.  输出的集合为 $N \cup\{c\}$.    
   若 $P(h)=c$  ,  c是chance node,  则在$h$ 之后随机选择一个action.   
   显然, $h$ 要能体现出这一轮谁行动.   
6. **chance**节点的行动函数: $f_{c}$ ,  在$P(h)=c$之后, 随机选择一个action, 则按照这个函数;  $f_{c}(a \vert h)$ 表示给定h后 aciton a发生的几率.  每个这样的概率度量probability measure  $f_{c}(\cdot \vert h)$ on $A(h)$ 都是独立于其他的. 即各个h的f独立.
7. 对于每一个玩家$i \in N$ ,  存在 $$\{h \in H: P(h)=i\}$$  (所有可能遭遇的历史集合) 的一个划分 $$\mathcal{I}_{i}$$ ，只要$h$和$h′$ 在划分的同一成员$I_i$中，则 $A(h)=A(h′)$ ,  可选的action的空间一样.     即,对玩家i无法区分.   
   对于$$I_i \in \mathcal{I}_{i}$$，我们用 $$A\left(I_{i}\right)$$表示集合 $A(h)$ ，用 $$P\left(I_{i}\right)$$ 表示任意 $$h \in I_{i}$$的玩家$P(h)$。  
   $\mathcal{I}_{i}$是玩家i的**信息划分 information partition**；集合 $$I_{i} \in \mathcal{I}_{i}$$ 是玩家i的**信息集 information set**。
8. 对于每一个玩家$i \in N$ , 有一个**收益函数 效用函数utility function** $u_{i}$ , 映射终结状态$Z$ 到实数集**收益** $\mathbf R$ . 
9. 若 $N=\{1,2\}$ 且 $u_{1}=-u_{2}$, 则是 **zero-sum extensive game  零和博弈** 
10. 定义 $$\Delta_{u, i}=\max _{z} u_{i}(z)-\min _{z} u_{i}(z)$$  为玩家i的 收益区间。

需要注意的是，如上所述的信息划分 会导致玩家被迫忘记自己过去的决定，从而出现一些奇怪的、不真实的情况。如果所有的玩家都能回忆起自己以前的行动和相应的信息集，那么这个博弈就可以说是一个**完美回忆 perfect recall** 的博弈。本工作将重点研究具有完美回想的有限、零和的扩展博弈。



#### 2.1 Strategies

玩家$i$ 的策略strategy $\sigma_{i}$ 是一个在 $A\left(I_{i}\right)$ 上的概率分布函数.  $\Sigma_{i}$ 是玩家i的所有策略的集合. 

**strategy profile 策略组合** $\sigma$ :  由每个玩家的策略组合而成. $\sigma_{1}, \sigma_{2}, \ldots .$  
$\sigma_{-i}$ 表示 $\sigma$ 的所有策略除了 $\sigma_{i} .$

如果玩家们根据策略$\sigma$ 来执行action, 则生成的轨迹$h$的概率分布为: $\pi^{\sigma}(h)$  , 注意这个不是某个action的概率, 是action序列--历史$h$ 的出现几率.     **Reach概率**

可以拆解为每个玩家的**贡献**  $\pi^{\sigma}=\Pi_{i \in N \cup\{c\}} \pi_{i}^{\sigma}(h)$  
$\pi_{i}^{\sigma}(h)$ 表示这样一个概率:  玩家$i$ 按照 $\sigma$ 执行, 在轨迹$h$ 中, 所有轮到自己行动的时候$P\left(h^{\prime}\right)=i$,  即在所有的$h'$处  (显然$h'$是 $h$ 某个前缀) ,  玩家$i$选择的 action 属于$h$ 的几率;    
其实就是在整个动作序列里面, 属于玩家i 按策略走, 轨迹正好是h的几率, 每次选择action符合h的几率之积.  
$\pi_{-i}^{\sigma}(h)$  表示不含玩家i的其他所有玩家的贡献的积. 

对 $I \subseteq H$,  定义  $\pi^{\sigma}(I)=\sum_{h \in I} \pi^{\sigma}(h)$ 为按照 $\sigma$ 遇到某个信息集的概率,  就是该集合中所有历史被遇到的概率**之和**, 即只遇到这个集合中的一个, 都算是遇到了这个信息集 ; 同上也可以定义 $\pi^{\sigma}_i(I)$ 和 $\pi^{\sigma}_{-i}(I)$  , 两部分的贡献.

执行策略组合$\sigma$ 的玩家$i$的期望回报 , $u_{i}(\sigma)=\sum_{h \in Z} u_{i}(h) \pi^{\sigma}(h) .$



#### 2.2 Nash Equilibrium

传统求解双人博弈的核心就是 **纳什均衡**. 

**纳什均衡**是一个策略组合$\sigma$ : 

$$
u_{1}(\sigma) \geq \max _{\sigma_{1}^{\prime} \in \Sigma_{1}} u_{1}\left(\sigma_{1}^{\prime}, \sigma_{2}\right) \quad u_{2}(\sigma) \geq \max _{\sigma_{2}^{\prime} \in \Sigma_{2}} u_{2}\left(\sigma_{1}, \sigma_{2}^{\prime}\right)
$$

即, 玩家随便采取一个不同于$\sigma$的策略, 都会导致期望回报下降. 

**近似纳什均衡** or $\epsilon$ -纳什均衡 :  离纳什均衡有段距离， 没达到最佳 ； 如果可以通过迭代逼近， 即 $\epsilon$ 趋向0，则该策略趋向均衡。
$$
u_{1}(\sigma)+\epsilon \geq \max _{\sigma_{1}^{\prime} \in \Sigma_{1}} u_{1}\left(\sigma_{1}^{\prime}, \sigma_{2}\right) \quad u_{2}(\sigma)+\epsilon \geq \max _{\sigma_{2}^{\prime} \in \Sigma_{2}} u_{2}\left(\sigma_{1}, \sigma_{2}^{\prime}\right)
$$

也可以说， 任何一个玩家怎么折腾， 也无法获得超过 $\epsilon$ 的收益。



#### 2.3 Regret Minimization 

这里直接应用到  extensive game。

**遗憾Regret** 是一个**在线学习**的概念，引发了一系列强大的学习算法。要定义这个概念，首先考虑**重复玩一个扩展博弈**, 每轮玩的时候, 策略可以不一样,   $σ_i^t$是**玩家i在第t轮round中使用的策略**。 

玩家i在时间T时的**平均总遗憾average overall regret**是 :

$$
R_{i}^{T}=\frac{1}{T} \max _{\sigma_{i}^{*} \in \Sigma_{i}} \sum_{t=1}^{T}\left(u_{i}\left(\sigma_{i}^{*}, \sigma_{-i}^{t}\right)-u_{i}\left(\sigma^{t}\right)\right)
$$

就是把每次 由于没采取最好的策略的regret累加起来.  regret是最优解减去当前解, 所以就是得知道最优解, Regret Minimization是针对策略博弈， 所以只有一个回合，很容易知道最优解；但要将该定理扩展到extensive博弈，就要对game tree 遍历.  

累积regret就可以, 不用平均. 但累计regret会随着T的增大趋向无穷大。

另外, 定义玩家i从时间1到T的**平均策略 average strategy** $$\bar{\sigma}_{i}^{t}$$ . 特别的, 对每个信息集  $$I \in \mathcal{I}_{i}$$ , 对每个$a \in A(I)$,   **<mark>注意, 这里求平均策略,要乘以 自己的 reach prob.</mark> 很多代码里都是直接求平均**

$$
\bar{\sigma}_{i}^{t}(I)(a)=\frac{\sum_{t=1}^{T} \pi_{i}^{\sigma^{t}}(I) \sigma^{t}(I)(a)}{\sum_{t=1}^{T} \pi_{i}^{\sigma^{t}}(I)}
$$

这里, 因为每轮策略$\sigma$ 是迭代更新的, 所以策略有个时间t的版本上标 $\sigma^t$ , 所以也就有了平均策略的概念,  即总体上采取a的平均概率. 

分母是一个信息集I，T次中每次出现的概率的和，显然对于normal-form， 这个就是整数T； 分子则是T次中， 每次在I下采用a的概率的和。

这里并没有说T次中，每次的 $\sigma^t$ 是如何来取得的。即Regret Matching。





遗憾与纳什均衡解之间存在着一个著名的联系。

##### Theorem 2

*In a zero-sum game at time $T,$ if both player's average overall regret is less than $\epsilon$, then $\bar{\sigma}^{T}$ is a $2 \epsilon$ equilibrium.*  在一个零和博弈中，在T时刻，如果双方玩家的平均遗憾值都小于ε，则$\bar{\sigma}^{T}$ 为一个 $2 \epsilon$ 均衡。

玩家$i$ 选择 $\sigma_{i}^{t}$ ，如果玩家$i$的平均总遗憾随着$t$增大而趋向0 (不考虑序列$$\sigma_{-i}^{t}$$)，则该算法为**最小遗憾 regret minimizing** 算法。

因此，self-play中的遗憾最小化算法可以作为一种**计算近似纳什均衡**的技术。 此外，算法对平均总遗憾的约束是对逼近的收敛率的约束。

传统上，遗憾最小化主要集中在更类似于 正则型normal-form博弈的bandit问题上。虽然在概念上可以将任何有限扩展博弈转换为等效的正则型博弈，但由于表示representation的大小呈指数级增长，使得使用遗憾算法不切实际。  
最近，Gordon提出了（Lagrangian Hedging，LH）系列算法，配合realization plan representation，可以在扩展博弈中最大限度地减少遗憾[5] .   
我们还提出了一种利用扩展博弈的紧凑性compactness 的遗憾最小化程序。我们的技术不需要使用LH所需要的昂贵的二次方程优化，这使得它可以更容易地扩展，同时实现了更严格的遗憾边界。



### 3 Counterfactual Regret

我们的方法的**基本思想是将总体遗憾分解成一组加法遗憾项，并将其独立最小化**。特别是，我们为扩展博弈引入了一个新的遗憾概念，称为**虚拟遗憾**，它是在**单个信息集**上定义的。我们展示了总体遗憾受到虚拟遗憾之和的约束，同时也展示了虚拟遗憾如何在每个信息集上独立地最小化。

我们首先考虑一个特定的信息集$$I \in \mathcal{I}_{i}$$和玩家i在该信息集 上的选择.  
定义 $u_i(\sigma, h)$ 为玩家i的期望收益,  给定已经达到的历史$h$ , 然后所有的玩家都执行策略$\sigma$ .   
将**虚拟收益counterfactual utility** $u(\sigma, I)$ 定义为：达到信息集$I$，除了玩家i外所有玩家遵循策略$\sigma$ , 得到预期的收益.  这可以被认为是 "虚拟"的，因为它是玩家i 试图达到信息集I的值。  
最后，对所有  $a \in A(I)$，定义$\left.\sigma\right|_{I \rightarrow a}$为 一个与$\sigma$相同的策略组合，除了玩家i总是选择行动a. 

则**即时虚拟遗憾 immediate counterfactual regret**为: 

$$
R_{i, \operatorname{imm}}^{T}(I)=\frac{1}{T} \max _{a \in A(I)} \sum_{t=1}^{T} \pi_{-i}^{\sigma^{t}}(I)\left(u_{i}\left(\left.\sigma^{t}\right|_{I \rightarrow a}, I\right)-u_{i}\left(\sigma^{t}, I\right)\right)
$$

与某个信息集$I$ 相关的 当时的 regret  ,  感觉叫 某个node的  local regret 更好理解. 

为什么叫 immediate , 即时, 是指这次迭代的regret, 相当于accumulate regret而言.

在扩展博弈中, immediate CFR 要考虑cfr prob, 即外部影响的到达某个node的概率.

直观地讲，这是玩家在信息集$I$ 上的决策的regret，用虚拟收益来表示，再加一个虚拟概率权重项(表示在t回合$I$ 能遇到的概率, 如果玩家尝试去做;  这里去掉了i的贡献) 。

我们通常关心regret什么时候是正的. 令 $R_{i, \operatorname{imm}}^{T+}(I)=\max \left(R_{i, \operatorname{imm}}^{T}(I), 0\right)$ . 

下面描述第一个关键结论. 



##### Theorem 3

 证明见附录.   重要结论, 收敛到均衡的证明. 

$$
R_{i}^{T} \leq \sum_{I \in \mathcal{I}_{i}} R_{i, \operatorname{imm}}^{T+}(I)
$$

由于最小化直接虚拟遗憾，使总体遗憾最小化，因此，如果我们只将直接虚拟遗憾最小化，就能找到一个近似的纳什均衡。

即时虚拟遗憾的关键特征是，只需控制 $\sigma_{i}(I)$ 就可以将其最小化。为此，我们可以使用Blackwell的逼近算法，在每个信息集上独立地将这个遗憾最小化。

 In particular, we maintain for all $I \in \mathcal{I}_{i}$, for all $a \in A(I)$ :

$$
R_{i}^{T}(I, a)=\frac{1}{T} \sum_{t=1}^{T} \pi_{-i}^{\sigma^{t}}(I)\left(u_{i}\left(\left.\sigma^{t}\right|_{I \rightarrow a}, I\right)-u_{i}\left(\sigma^{t}, I\right)\right)
$$

表示, 每轮在 $I$上都选a, 相比之前的策略, 造成的收益的提升,即regret. 

**核心迭代算法**:

Define $R_{i}^{T,+}(I, a)=\max \left(R_{i}^{T}(I, a), 0\right),$ then the strategy for time $T+1$ is:

$$
\sigma_{i}^{T+1}(I)(a)=\left\{\begin{array}{ll}
\frac{R_{i}^{T, +}(I, a)}{\sum_{a \in A(I)} R_{i}^{T, +}(I, a)} & \text { if } \sum_{a \in A(I)} R_{i}^{T,+}(I, a)>0 \\
\frac{1}{|A(I)|} & \text { otherwise }
\end{array}\right. \tag{7}
$$

分子分母的$R^{T+}$ 会约掉 $\frac1T$ , 所以程序求$R^T$的时候可以不除以T; 理论分析的时候需要.

换句话说，行为是按照  因为不选择该action造成的虚拟遗憾量的比例 来选择的。如果所有行动都没有正的遗憾值，那么就随机选择。

这就引出了我们的第二个关键结果。



##### Theorem 4 

If player i selects actions according to Equation 7 then $$R_{i, \text { imm }}^{T}(I) \leq \Delta_{u, i} \sqrt{\left \vert A_{i}\right\vert } / \sqrt{T}$$ and consequently $$R_{i}^{T} \leq \Delta_{u, i}\left \vert \mathcal{I}_{i}\right\vert \sqrt{\left\vert A_{i}\right\vert} / \sqrt{T}$$ where $$\left\vert A_{i}\right\vert=\max _{h: P(h)=i}\vert A(h)\vert$$

证明见附录. 这个结果确定了方程7中的策略可以用于self-play中计算出纳什均衡。此外，在信息集的数量上，平均总遗憾的约束是线性的。这些约束与Gordon的Lagrangian Hedging算法所能达到的约束相似。同时，最小化虚拟遗憾不需要在每次迭代时进行昂贵的二次函数投影。   

公式看出, 遗憾是越来越小的. 

在下一节中，我们将在扑克领域演示我们的技术。



### 4 Application To Poker

现在我们介绍一下我们如何利用虚拟最小化来计算扑克领域中的近似纳什均衡解。我们关注的扑克变体是heads-up limit德州扑克，因为它被用于AAAI计算机扑克比赛[9]。该游戏由两个玩家（零和）、四轮发牌和四轮下注组成，游戏状态略低于$10^{18}$种[2]。与之前关于这个领域的所有工作一样，我们将首先对博弈进行抽象，并找到一个抽象的博弈均衡。用扩展博弈的术语表示，我们将合并信息集；在扑克的术语中，我们将分组bucket纸牌序列。由此得到的近乎均衡解的质量取决于抽象粒度的粗细。一般来说，抽象程度越低，所得到的策略的质量越高。因此，能够解决一个较大的博弈的能力意味着需要更少的抽象，从而转化为更强的扑克游戏程序。



#### 4.1 Abstraction

**抽象的目标是将每个玩家的信息集的数量减少到一个可执行的大小**，这样抽象的游戏就可以被解决。早期的扑克抽象[2，4]涉及到限制可能的下注顺序，例如，每轮只允许三次下注，或者用固定的策略代替所有的第一轮决策。最近，涉及到每轮完整的四次下注的扩展博弈的抽象已经被证明是一个显著的改进[7，6]。我们也将保留完整的下注结构，将抽象的重点放在发牌上。

我们的抽象是基于 牌力平方hand strength squared 的度量标准，将观察到的牌序分组在一起。牌力是给定一个玩家所看到的牌的预期胜率。这在之前的抽象工作中被大量使用[2，4]。牌力平方是只给定了玩家看到的牌，在最后一张牌被揭晓后的预期牌力平方。直观地讲，牌力平方与牌力相似，但给最终牌力方差较大的牌序以奖励。更高的方差是首选，因为这意味着玩家在摊牌之前，最终会更确定自己的最终胜算。更重要的是，我们将在第5节中表明，这种抽象度量会导致更强的扑克策略。

最后的抽象是根据 牌力平方度量来划分纸牌序列。首先，将所有的round-one card sequences（即私人手里的2张）根据该度量划分为十个大小相等的bucket。然后，所有round-two card sequences , 也用第一轮的那种分组bucket分为10组，根据第二轮牌力的度量。因此，card sequences的分组在第二轮后是一对数字：上一轮的bucket和本轮中的bucket。再之后的轮次继续这么做, 得到**bucket sequences**：每轮从{1，.....10}中分出一个桶。由此得到的抽象游戏大约有$1.65×10^{12}$个游戏状态，以及$5.73×10^7$个信息集。在完整的扑克游戏中，大约有$9.17×10^{17}$个游戏状态和$3.19×10^{14}$个信息集。因此，尽管这代表了对原始游戏的重要抽象，但比之前解决的抽象要大两个数量级。



#### 4.2 Minimizing Counterfactual Regret

现在，我们已经指定了一个抽象，下面用 CFR 最小化来计算这个游戏的近似均衡。基本程序是让两个玩家使用方程7中的策略反复进行博弈。在游戏的T次重复之后，或者说是简单的迭代之后，我们将返回$$\left(\bar{\sigma}_{1}^{T}, \bar{\sigma}_{2}^{T}\right)$$ 作为得到的近似均衡。重复play 需要为每个信息集 $I$ 和动作 $a$ 存储$R_{i}^{t}(I, a)$，并在每次迭代后更新.



在我们的实验中，实际上使用了这个基本程序的变体，它利用了我们的抽象相对于博弈状态的数量而言，信息集数量较少的事实。尽管每个信息集都很关键，但许多信息集都是由一百个或更多的独立历史组成。这一事实表明，只需要对相关博弈状态的一小部分进行采样，就可以很好地了解一个信息集的正确行为。特别地，对于每一次迭代，我们对chance玩家的确定性行为进行采样。因此，设置 $\sigma_{c}^{t}$ 为确定性策略，但根据 $f_{c}$ 的分布来选择。对于我们的抽象来说，这相当于为两个玩家选择了一个联合bucket序列。一旦指定了联合bucket序列，就只有 18,496 个可到达的状态和 6,378 个可到达的信息集。 因为$\pi_{-i}^{\sigma^{t}}(I)$ 是0, 所以没必要在这些信息集上更新. 

这个采样变体版本可以在2.4Ghz双核AMD Opteron 280处理器的单核上，在一秒钟内完成大约750次的算法迭代。此外，直接并行化也是可能的，并且在实验中就使用了这种直接并行化。由于下注是公共信息，所以可以独立计算出特定的翻牌前下注序列的翻牌信息集。用四个处理器，我们能够在一秒钟内完成大约1700次迭代。完整的算法细节和伪代码可以在附录中找到。



###  5 Experimental Results



先考虑一下如何评估一个接近平衡的扑克策略的强度。一个自然的方法是衡量策略的可利用性exploitability，或者说是衡量策略在最坏情况下的对手的表现。在一个对称的零和游戏中，比如heads-up，一个完美的均衡策略的可利用性为零，$\epsilon$ -Nash equilibrium 的可利用性为$\epsilon$ 。一个方便的衡量可利用性的指标是millibets-per-hand (mb/h)，其中millibet是小注的千分之一，是前两轮投注中使用的固定下注量。为了给这些数字提供一些直觉，一个总是fold的玩家将输掉750 mb/h ; 而一个玩家比另一个强10 mb/h ，就需要超过100万手，95％的把握赢才行。

一般来说，在完整的博弈中计算出一个策略的可利用性是很难的。对于合理抽象过的策略，可以在抽象博弈中计算出可利用性。这样的衡量标准是对生成均衡策略的技术的有用评估。但是，这并不意味着该技术不能被抽象之外的策略所利用。因此，将该策略在完整游戏中的性能与已知的强大程序的进行比较是很常见的。虽然对阵对手的正收益不具有可传递性，但在对阵大量不同的对手时获胜，表明是强势程序。

我们使用采样的CFR最小化程序来为我们的抽象博弈找到一个近似均衡，如上一节所述。该算法运行了20亿次迭代$\left(T=2 \times 10^{9}\right)$，即在4个CPU上并行化时，计算时间不到14天。由此得出的策略在其自身抽象博弈中的可利用性为2.2 mb/h。只经过2亿次迭代，即不到2天的计算，该策略的可利用性已经不到13 mb/h。注意，算法每次迭代只访问了18,496个博弈状态。经过2亿次迭代后，每个游戏状态平均被访问了不到2.5次，但算法已经计算出了一个相对准确的解。



##### 5.1 Scaling the Abstraction

尝试了一些规模更小的抽象。这些抽象每轮使用较少的bucket来分组纸牌序列。除了10个bucket，试了8个、6个和5个的变体。由于这些抽象量较小，它们需要较少的迭代来计算出一个非常精确的均衡。例如，CFR5 计算量大约是小了250倍，只有不到$10^{10}$个博弈状态。经过1亿次迭代，或33个小时的计算，在没有任何并行化的情况下，最终的策略可利用性为 3.4 mb/h。这与最先进的算法[6，7]用许多天的计算量解决的游戏规模大小差不多。

<img src="/img/2020-04-14-CFR.assets/image-20200514015655580.png" alt="image-20200514015655580" style="zoom:50%;" />



图1b显示了抽象的收敛率 。x轴为信息集数量的归一化后的迭代次数,  几个变体的收敛率几乎完全一致，这表明在实践中，所需的迭代次数是随着信息集的数量而线性增长的。由于使用了采样bucket序列，每次迭代的时间几乎与抽象的大小无关。这表明，在实践中，总体计算复杂度仅与所选的纸牌抽象的大小呈线性关系。



#### 5.2 Performance in Full Texas Hold’em

能够解决更大的游戏意味着需要更少的抽象化，从而产生一个整体更强的扑克游戏程序。

<img src="/img/2020-04-14-CFR.assets/image-20200514020423710.png" alt="image-20200514020423710" style="zoom:50%;" />







### Conclusion

我们提出了一个新的后悔概念，用于扩展博弈，称为虚拟后悔。我们证明了虚拟后悔最小化可以使总体后悔最小化，并提出了一种通用的、特定于扑克的虚拟后悔有效最小化的算法。我们在扑克领域中演示了该技术，表明该技术可以计算出多达$10^{12}$个状态的抽象的近似平衡，比以前的方法大两个数量级。我们还表明，所得到的扑克牌游戏程序的性能优于其他程序，包括2006年AAAI计算机扑克大赛的庄家部分的所有竞争对手。



### A.4 Poker-Specific Implementation

我们需要迭代所有可以到达的信息集，并计算概率和regret。为了快速完成这个任务，我们用 "player view tree"来表示每个信息集中的数据：换句话说，我们从不显式地表示抽象的游戏中的每个状态：相反，我们用自己的树来表示每个玩家的信息集，每个节点n是四种类型中的一种。

- Bucket Nodes: 表示被观察的纸牌的信息的节点。对每一个不同的类别的节点（对手或玩家节点）都有一个子节点（对手或玩家节点）。
- Opponent Nodes:  代表对手采取动作的地方的节点. 每一个动作都有一个子节点。
- Player Nodes:  代表当前玩家采取的行动的节点。包含每个动作的平均遗憾值，直到这个点的每个动作的总概率，以及每个动作的子节点（可以是Opponent、Bucket或Terminal节点）。有一个与这个节点相关的隐含信息集，我们将把它写成$I(n)$ . 
- Terminal Nodes: 有人folding or showdown.  给定赢、输、平局的概率，有足够的信息来计算出一个预期收益。



每个玩家观察到不同的游戏信息，因此在计算过程中，会遍历其game tree的不同部分。我们的算法以配对的方式在两个树上递归。在开始之前，我们先定义  $u_{i}^{\prime}(\sigma, I)=\pi_{-i} u_{i}(\sigma, I)$。对于树中的每一个节点，都会有一个值，$u_{i}(\sigma, n)$ 我们用这个值来计算 $u_{i}(\sigma, I)$ and $u_{i}(\sigma, I, a)$，它是给定信息集I达到并采取行动a的预期值。



<img src="/img/2020-04-14-CFR.assets/image-20200514023722939.png" alt="image-20200514023722939" style="zoom:50%;" />



### A.2 Regret Matching

In general, **regret matching** can be defined in a domain where there are a fixed set of actions $A,$ a function $u^{t}$ $A \rightarrow \mathbf{R},$ and on each round a distribution over the actions $p^{t}$ is selected.
Define the regret of not playing action $a \in A$ until time $T$ as:
$$
R^{t}(a)=\frac{1}{T} \sum_{t=1}^{T} u^{t}(a)-\sum_{a \in A} p^{t}(a) u^{t}(a)
$$

上式 后面应该有个大括号， 减号后面, 就是每次游戏的实际 收益(期望);  第一个连加后面整体， 是 t轮次的regret，     R就是T次游戏后, 关于a的regret sum的平均,  除以T要不要无所谓.



and define $R^{t,+}(a)=\max \left(R^{t}(a), 0\right)$ .   To apply regret matching, one chooses the distribution:
$$
p^{t}(a)=\left\{\begin{array}{ll}
\frac{R^{t-1,+}(a)}{\sum_{a^{\prime} \in A} R^{t-1,+}\left(a^{\prime}\right)} & \text { if } \sum_{a^{\prime} \in A} R^{t-1,+}\left(a^{\prime}\right)>0 \\
\frac{1}{|A|} & \text { otherwise }
\end{array}\right.
$$



**Theorem 7** If $|u|=\max _{t \in\{1 \ldots T\}} \max _{a, a^{\prime} \in A}\left(u^{t}(a)-u^{t}\left(a^{\prime}\right)\right),$ the regret of the regret matching algorithm is
bounded by:
$$
\max _{a \in A} R^{t}(a) \leq \frac{|u| \sqrt{|A|}}{\sqrt{T}}
$$

Blackwell's original result [?] focused on the case where an action (or vector) is chosen at random (instead of a distribution over actions) and gave a probabilistic guarantee. The result above focuses on the distributions selected, and is more applicable to a scenario where a probability is selected instead of an action.







## Reference

Regret Minimization in Games with Incomplete Information   







