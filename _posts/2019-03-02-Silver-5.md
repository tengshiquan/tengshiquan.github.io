---
layout:     post
title:      UCL Course on RL,  Function Approximation
subtitle:   David Silver çš„è¯¾ç¨‹ç¬”è®°5
date:       2019-03-02 12:00:00
author:     "tengshiquan"
header-img: "img/post-atari.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning
    - DavidSilver
    - UCL
---



# Value Function Approximation

æœ¬ç« æ ¸å¿ƒæ˜¯DQN åŠä¸¤ä¸ªæŠ€å·§

1. Introduction
2. **Incremental** Methods
3. **Batch** Methods





### Large-Scale Reinforcement Learning

- Backgammon: $10^{20}$ states
- Computer Go: $10^{170}$ states
- Helicopter: continuous state space

scale up  to å¤§è§„æ¨¡MDPé—®é¢˜



### Value Function Approximation   å‡½æ•°é€¼è¿‘

- So far we have represented value function by a **lookup table**

  - Every state s has an entry V(s) Or every state-action pair s,a has an entry Q(s,a)

- Problem with large MDPs:

  - There are **too many** states and/or actions to **store** in memory
  - It is **too slow** to **learn** the value of each state individually

- Solution for large MDPs:

  - Estimate value function with **function approximation**
    $$
    \hat v(s, \mathbf w) \approx v_\pi(s) \quad or \quad \hat q(s, a,\mathbf w) \approx q_\pi(s,a)
    $$
  
- **Generalise** from seen states to unseen states   æ³›åŒ–
  
- **Update parameter w using MC or TD learning**  ä¸æ˜¯å»æ›´æ–°tableï¼Œè€Œä¸”æ˜¯æ›´æ–°æ‹Ÿåˆå‡½æ•°çš„å‚æ•°





##### Types of Value Function Approximation

<img src="/img/2019-03-02-Silver.assets/image-20190203224215926.png" style="zoom: 60%;" />

1. é’ˆå¯¹çŠ¶æ€æœ¬èº«ï¼Œè¾“å‡ºè¿™ä¸ªçŠ¶æ€çš„è¿‘ä¼¼ä»·å€¼ï¼› 

2. é’ˆå¯¹çŠ¶æ€è¡Œä¸ºå¯¹ï¼Œè¾“å‡ºçŠ¶æ€è¡Œä¸ºå¯¹çš„è¿‘ä¼¼ä»·å€¼ï¼› **action in**

3. é’ˆå¯¹çŠ¶æ€æœ¬èº«ï¼Œè¾“å‡ºä¸€ä¸ªå‘é‡ï¼Œå‘é‡ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ æ˜¯è¯¥çŠ¶æ€ä¸‹é‡‡å–ä¸€ç§å¯èƒ½è¡Œä¸ºçš„ä»·å€¼ã€‚ **action out**

æ˜¾ç„¶ç¬¬ä¸‰ç§åœ¨ä½¿ç”¨çš„æ•ˆç‡ä¸Šæ˜¯å¾ˆé«˜çš„ï¼Œä½†åº”è¯¥æ¯”å‰é¢çš„éš¾è®­ç»ƒ



#### function approximators

- **Linear** combinations of features
- **Neural network**
- Decision tree
- Nearest neighbour 
- Fourier / wavelet bases 
- ... 

1. **differentiable** function approximators  å¯å¯¼çš„, å¯ä»¥ä½¿ç”¨æ¢¯åº¦
2. Furthermore, we require a training method that is suitable for **non-stationary, non-iid** data  
   MDPè¿‡ç¨‹çš„æ•°æ®ï¼Œéé™æ€,  éç‹¬ç«‹åŒåˆ†å¸ƒï¼Œå‰åçŠ¶æ€ä¹‹é—´å…³è”æ€§å¾ˆé«˜ï¼Œç›‘ç£å­¦ä¹ ä¸å®Œå…¨é€‚åˆï¼›ç›‘ç£å­¦ä¹ çš„æ•°æ®å¯ä»¥è§†ä¸ºiid





## Incremental Methods 

æ¯èµ°ä¸€æ­¥è·å¾—æ–°æ•°æ®ï¼Œç„¶åonlineæ›´æ–°value fuction

#### Gradient Descent

- æ¢¯åº¦ä¸‹é™ç®—æ³•
- Let $J(\mathbf  w)$ be a differentiable function of parameter vector $\mathbf w$
- **gradient**

$$
\nabla_{\mathbf w}J(\mathbf w) =  \begin{pmatrix}
 \frac{\partial J(\mathbf w)}{\partial  \mathbf w_1}\\
\vdots\\
\frac{\partial J(\mathbf w)}{\partial  \mathbf w_n}\\
\end{pmatrix}
$$

- To find a local minimum of $J(\mathbf w)$ , Adjust $\mathbf w$ in direction of -ve gradient

$$
\Delta \mathbf w = - \frac{1}{2} \alpha \nabla_{\mathbf w}J(\mathbf w)
$$

##### Stochastic Gradient Descent  SGD

ç›®æ ‡å‡½æ•°ä»¥åŠæ¢¯åº¦, ç›‘ç£å­¦ä¹ 

- Goal: find parameter vector $\mathbf w$ minimising **mean-squared error**  ,  mse

$$
J(\mathbf w) = \mathbb E_\pi[(v_\pi(S) - \hat v(S,\mathbf w))^2]
$$

- Gradient descent finds a **local minimum**

$$
\Delta \mathbf w = - \frac{1}{2} \alpha \nabla_{\mathbf w}J(\mathbf w) 
= \alpha \mathbb E_\pi[(v_\pi(S) - \hat v(S,\mathbf w)) \nabla_{\mathbf w}\hat v(S,\mathbf w)]
$$

- **Stochastic gradient descent** **samples** the gradient

$$
\Delta \mathbf w =   \alpha  (v_\pi(S) - \hat v(S,\mathbf w)) \nabla_{\mathbf w}\hat v(S,\mathbf w)
$$

- **Expected update is equal to full gradient update**,  full æ„å‘³ç€åœ¨å…¨éƒ¨æ•°æ®ä¸Šåšä¸€æ¬¡æ¢¯åº¦ 



### Linear Function Approximation

ç›‘ç£å­¦ä¹ ä¸­, çº¿æ€§æ‹Ÿåˆ, å¯ä»¥æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜, å°±æ˜¯æ‰¾åˆ°æœ€ä½³çš„çº¿æ€§æ‹Ÿåˆå‡½æ•°. 

- Represent state by  **Feature Vectors**

$$
\mathbf x(S)  =  \begin{pmatrix}
\mathbf x_1 (S)\\
\vdots\\
\mathbf x_n (S)\\
\end{pmatrix}
$$

- Represent value function by a linear combination of features 

$$
\hat v(S,\mathbf w) = \mathbf x(S)^\top \mathbf w = \sum_{j=1}^n \mathbf x_j(S)\mathbf w_j
$$

- Stochastic gradient descent **converges** on **global optimum** 

$$
\Delta \mathbf w =   \alpha  (v_\pi(S) - \hat v(S,\mathbf w)) \mathbf x(S)
$$

- Update = step-size Ã— prediction error Ã— feature value
- çº¿æ€§Væ‹Ÿåˆ,  æ˜¯ç›‘ç£å­¦ä¹ , è®­ç»ƒtargetæ˜¯å·²ç»æ˜¯evaluateå®Œæˆçš„ $v_\pi(s)$ 



##### ~~Table Lookup Features~~

- One-hot è¡¨ç¤ºstateï¼Œæ¯ä¸ªstateå¯¹åº”ä¸€ä¸ªindex,  å¯¹çŠ¶æ€è¶…çº§å¤šçš„æƒ…å†µï¼Œä¼šè¶…çº§é•¿, ä¸å®ç”¨
- Table lookup is a special case of linear value function approximation
- Using table lookup features

$$
\mathbf x^{table}(S)  =  \begin{pmatrix}
\mathbf 1 (S = s_1)\\
\vdots\\
\mathbf 1 (S = s_n)\\
\end{pmatrix}
$$

- Parameter vector $\mathbf  w$ gives value of each individual state

$$
\hat v(S,\mathbf w)  =  \begin{pmatrix}
\mathbf 1 (S = s_1)\\
\vdots\\
\mathbf 1 (S = s_n)\\
\end{pmatrix} \cdot 
\begin{pmatrix}
\mathbf w_1\\
\vdots\\
\mathbf w_n\\
\end{pmatrix}
$$



### Incremental Prediction Algorithms

- **å¯¹predictioné—®é¢˜ï¼Œå…¨å±€æœ€ä¼˜åªæ˜¯æŒ‡çš„æ˜¯ æ‰¾åˆ°æœ€å¥½çš„çº¿æ€§æ‹Ÿåˆå‡½æ•°ï¼Œå…¨ä½“æ•°æ®ä¸Šè¯¯å·®æœ€å°çš„ï¼Œ åªéœ€GDï¼Œè°ƒæ•´å¥½å­¦ä¹ ç‡ï¼Œè‚¯å®šå¯ä»¥åœ¨å…¨ä½“æ ·æœ¬ä¸Šæ‰¾åˆ°è¯¯å·®æœ€å°çš„**  è¿™ä¸ªè¿˜å¥½ï¼Œéš¾çš„æ˜¯ä¸‹é¢çš„control



- Have assumed **true** value function $v_Ï€(s)$ given by supervisor ; ä¹‹å‰å‡è®¾å·²ç»æœ‰äº†OracleçœŸå€¼v ,ä¸ç°å®
- But in RL there is no supervisor, only rewards  ;    ä½†RLé‡Œé¢çš„vè¦å»ç®—
- In practice, we substitute a **target** for $v_Ï€(s)$  ;   æ²¡æœ‰çœŸå€¼ï¼Œ**å…ˆå¼ºåŒ–å­¦ä¹ å†ä»£å…¥GDå…¬å¼**
  - MC : $\Delta \mathbf w = \alpha ({\color{red}{G_t}} - \hat v(S_t,\mathbf w) )\nabla_{\mathbf w} \hat v(S_t,\mathbf w)$ 
  - TD(0) : $\Delta \mathbf w = \alpha ({\color{red}{R_{t+1}+\gamma \hat v(S_{t+1},\mathbf w)}} - \hat v(S_t,\mathbf w) )\nabla_{\mathbf w} \hat v(S_t,\mathbf w)$ 
  - TD($\lambda$) : $\Delta \mathbf w = \alpha ( {\color{red}{G^\lambda_t}} - \hat v(S_t,\mathbf w) )\nabla_{\mathbf w} \hat v(S_t,\mathbf w)$



#### Monte-Carlo with Value Function Approximation

- Return $G_t$ is an **unbiased**, **noisy** sample of true value $v_Ï€(S_t)$   
  Gæ˜¯Vçš„ä¸€ä¸ªé‡‡æ ·  æ— åæœ‰å™ªï¼Œå³æ— åæœ‰æ–¹å·®çš„
- Can therefore apply supervised learning to â€œtraining dataâ€: $âŸ¨S_1, G_1âŸ©, âŸ¨S_2, G_2âŸ©, ..., âŸ¨S_T , G_T âŸ©$
- For example, using linear Monte-Carlo policy evaluation

$$
\begin{align}
\Delta \mathbf w &= \alpha ({\color{red}{G_t}} - \hat v(S_t,\mathbf w) )\nabla_{\mathbf w} \hat v(S_t,\mathbf w) \\
& = \alpha(G_t-\hat{v}(S_t, \mathbf{w})) \mathbf x(S_t)\\
\end{align}
$$

- çº¿æ€§æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜
- Monte-Carlo evaluation **converges** to a **local optimum** Even when using non-linear value function approximation   éçº¿æ€§ MC evaluation ä¹Ÿèƒ½**æ”¶æ•›**åˆ°ä¸€ä¸ªå±€éƒ¨æœ€ä¼˜



#### TD Learning with Value Function Approximation

é€šè¿‡TDæ¥æ”¶æ•›åˆ°valueçš„çœŸå€¼ï¼Œä½†ä¸­é—´bootstrapçš„æ—¶å€™ï¼Œç”¨åˆ°å€¼å‡½æ•°çš„è¿‘ä¼¼å€¼,   è¿™ä¸ªè¿‘ä¼¼å€¼æ˜¯æœ‰åçš„; æ¨¡å‹æœ¬èº«çš„è®­ç»ƒ, è‹¥æ¯æ¬¡éƒ½è®­ç»ƒåˆ°æ”¶æ•›, åˆ™èµ·åˆ°äº†è¿‘ä¼¼qtableçš„ä½œç”¨ï¼›æ¯bootstrapä¸€æ¬¡, target å°±ä¼šè·Ÿå‡†ä¸€äº›. 



- The **TD-target** $R_{t+1} + Î³\hat v(S_{t+1}, \mathbf w)$ is a **biased** sample of true value $v_Ï€(S_t)$  ,  TDtargetçš„å€¼ æœ‰å
- apply supervised learning to â€œtraining dataâ€: $âŸ¨S_1, R_2+\gamma \hat v(S_2,\mathbf w)âŸ©, âŸ¨S_2, R_3+\gamma \hat v(S_3,\mathbf w)âŸ©, ..., âŸ¨S_{T-1} , R_T âŸ©$

- using linear TD(0) : $\Delta \mathbf w = Î±({\color{red}{R_{t+1} + Î³\hat v(S_{t+1}, \mathbf w)}} âˆ’ \hat v(S, \mathbf w))âˆ‡_{\mathbf w} \hat v(S, \mathbf w) = Î±Î´ \mathbf x(S)$ ;   
  TD err ä¹˜ linear gradient

- Linear TD(0) **converges** (close) to **global optimum**   
  æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜, å°½ç®¡targetæ˜¯æœ‰åä¼°è®¡ï¼Œ å³è‚¯å®šèƒ½æ‹Ÿåˆåˆ°æœ€ç»ˆçš„å…¨å±€æœ€ä¼˜ç‚¹

- å¯¹äºTD ï¼Œæœ€åä¸€æ­¥çš„sampleè‚¯å®šæ˜¯å‡†çš„ï¼Œ ç„¶åä¸­é—´æœ‰åçš„sampleï¼Œ Rçš„éƒ¨åˆ†æ˜¯å‡†çš„ï¼› ä½†å¯¹äºä¸­é—´R=0çš„envï¼Œåªæœ‰æœ€åä¸€æ­¥æ˜¯å‡†ï¼Œä¸­é—´éƒ½æ˜¯æœ‰å



#### TD(Î») with Value Function Approximation

- The Î»-return $G_t^Î»$ is also a **biased** sample of true value $v_Ï€(s)$
- â€œtraining dataâ€: $âŸ¨S_1, G_1^\lambdaâŸ©, âŸ¨S_2, G_2^\lambdaâŸ©, ..., âŸ¨S_{T-1} , G_{T-1}^\lambda âŸ©$

- Forward view linear TD(Î»)

$$
\Delta \mathbf w = Î±( {\color{red}{G_{t}^\lambda}} âˆ’ \hat v(S, \mathbf w))âˆ‡_{\mathbf w} \hat v(S, \mathbf w) = Î±( {\color{red}{G_{t}^\lambda}} âˆ’ \hat v(S, \mathbf w))\mathbf x(S)
$$

- Backward view linear TD(Î»)

$$
\begin{align}
\delta_t &= R_{t+1} + \gamma \hat v(S_{t+1},\mathbf w) - \hat v(S_t,\mathbf w)
\\ E_t &= \gamma \lambda E_{t-1} + \mathbf x(S_t) 
\\ \Delta \mathbf w &= \alpha \delta_t E_t
\end{align}
$$

- Forward view and backward view linear TD(Î») are equivalent



### Incremental Control Algorithms

#### Control with Value Function Approximation

<img src="/img/2019-03-02-Silver.assets/image-20190220000024445.png" style="zoom: 50%;" />

- Policy evaluation **Approximate** policy evaluation, $q(Â·, Â·,\mathbf w) â‰ˆ q_Ï€$
- Policy improvement **Îµ-greedy policy improvement**



#### Action-Value Function Approximation

- ç›‘ç£å­¦ä¹ ï¼Œ è¿˜æ˜¯å…ˆè¦å¾—åˆ° true value q ï¼Œå³ oracle
- Approximate the action-value function $\hat q(S, A, \mathbf w) â‰ˆ q_Ï€(S, A)$
- Minimise **MSE** between approximate action-value fn $\hat q(S,A,\mathbf w)$ and true action-value fn $q_Ï€(S,A)$

$$
J(\mathbf w) = \mathbb E_\pi[(q_\pi(S,A) - \hat q(S,A,\mathbf w))^2]
$$

- Use **SGD** to find a **local minimum**

$$
-\frac{1}{2}\nabla_\mathbf w J(\mathbf{w})=(q_\pi(S, A)-\hat{q}(S, A, \mathbf{w}))\nabla_\mathbf w \hat{q}(S, A, \mathbf{w})\\
\Delta\mathbf{w}=\alpha (q_\pi(S, A)-\hat{q}(S, A, \mathbf{w}))\nabla_\mathbf w\hat{q}(S, A, \mathbf{w})
$$



##### Linear Action-Value Function Approximation

- feature vector

$$
\mathbf x(S,A)  =  \begin{pmatrix}
\mathbf x_1 (S,A)\\
\vdots\\
\mathbf x_n (S,A)\\
\end{pmatrix}
$$

- Represent action-value fn by linear combination of features

$$
\hat q(S,A,\mathbf w) = \mathbf x(S,A)^\top \mathbf w = \sum_{j=1}^n \mathbf x_j(S,A)\mathbf w_j
$$

- **SGD** update

$$
\nabla_\mathbf w \hat q(S,A,\mathbf w) = \mathbf x(S,A)
\\ \Delta \mathbf w =   \alpha  (q_\pi(S,A) - \hat q(S,A,\mathbf w)) \mathbf x(S,A)
$$



### Incremental Control Algorithms

- æ²¡æœ‰çœŸå€¼ï¼Œä½¿ç”¨ TD æ¥bootstrap
- Like prediction, we must substitute a target for $q_Ï€(S,A)$
  - MC : $\Delta \mathbf w = \alpha ({\color{red}{G_t}} - \hat q(S_t,A_t,\mathbf w) )\nabla_{\mathbf w} \hat q(S_t,A_t,\mathbf w)$
  - TD(0) : $\Delta \mathbf w = \alpha ({\color{red}{R_{t+1}+\gamma \hat q(S_{t+1},A_{t+1},\mathbf w)}} - \hat q(S_t,A_t,\mathbf w) )\nabla_{\mathbf w} \hat q(S_t,A_t,\mathbf w)$
  - TD($\lambda$) : $\Delta \mathbf w = \alpha ( {\color{red}{q^\lambda_t}} - \hat v(S_t,A_t,\mathbf w) )\nabla_{\mathbf w} \hat q(S_t,A_t,\mathbf w)$
  - Backward view linear TD(Î»)

$$
\begin{align}
\delta_t &= R_{t+1} + \gamma \hat q(S_{t+1},A_{t+1},\mathbf w) - \hat q(S_t,A_t,\mathbf w)
\\ E_t &= \gamma \lambda E_{t-1} + \nabla_\mathbf w \hat q (S_t,A_t,\mathbf w) 
\\ \Delta \mathbf w &= \alpha \delta_t E_t
\end{align}
$$



##### Linear Sarsa with Coarse Coding in Mountain Car

Tile Codingæ˜¯Coarse Codingçš„ä¸€ç§ï¼Œç‰¹åˆ«é€‚åˆç”¨äºå¤šç»´è¿ç»­ç©ºé—´, å¯ä»¥è§suttonçš„ä¹¦

##### Linear Sarsa with Radial Basis Functions in Mountain Car



##### Study of Î»: Should We Bootstrap?

<img src="/img/2019-03-02-Silver.assets/image-20200701151127227.png" alt="image-20200701151127227" style="zoom:50%;" />

- æ€»çš„æ¥è¯´**Î»=1çš„æ—¶å€™é€šå¸¸ç®—æ³•è¡¨ç°æ˜¯å¾ˆå·®**çš„, å³MCæ–¹æ³•çš„ç¡®æ”¶æ•›çš„æ…¢ 
- TD(0)æ˜¯æ¯”MCå¥½å¾—å¤šçš„æ–¹æ³•ï¼Œè¿™è¯´æ˜äº†Bootstrapçš„é‡è¦æ€§
- ä¸åŒçš„ä»»åŠ¡å¯¹åº”çš„æœ€ä¼˜Î»å€¼æ˜¯ä¸å¤ªå®¹æ˜“ç¡®å®šçš„ã€‚



### Convergence

Bootstrap æ˜¯å¾ˆå¥½ï¼Œ ä½†æ˜¯ä¸æ˜¯æ”¶æ•›å‘¢

##### Bairdâ€™s Counterexample

ä¾‹å­è¯´æ˜TD ä¸ä¿è¯æ”¶æ•› , è¯¦ç»†è§ sutton ä¹¦



#### Convergence of Prediction Algorithms

MCä½¿ç”¨çš„æ˜¯å®é™…ä»·å€¼çš„æœ‰å™ªå£°æ— åä¼°è®¡ï¼Œè™½ç„¶å¾ˆå¤šæ—¶å€™è¡¨ç°å¾ˆå·®ï¼Œä½†æ€»èƒ½æ”¶æ•›è‡³å±€éƒ¨æˆ–å…¨å±€æœ€ä¼˜è§£ã€‚TDæ€§èƒ½é€šå¸¸æ›´åŠ ä¼˜ç§€ï¼Œä½†TD ä¸æ”¶æ•›.  è¿™è¿˜åªæ˜¯ prediction, ä¸€èˆ¬åªçœ‹control. 

| On/Off-Policy | Algorithm                | Table Lookup    | Linear          | Non-Linear      |
| ------------- | ------------------------ | --------------- | --------------- | --------------- |
| On-Policy     | MC<br/>TD(0)<br/>TD(Î»)   | Y<br />Y<br />Y | Y<br />Y<br />Y | Y<br />N<br />N |
| Off-Policy    | MC<br />TD(0)<br />TD(Î») | Y<br />Y<br />Y | Y<br />N<br />N | Y<br />N<br />N |



##### Gradient Temporal-Difference Learning

åœ¨predictioné—®é¢˜ä¸­ï¼Œå¼•å…¥æ–°çš„ **Gradient TD**ç®—æ³•ï¼Œå¯ä»¥æ”¶æ•› ; ä½†è®¨è®ºä¸å¤š , ç›®å‰çœ‹æ¥è¯¥æ–¹å‘ä¸é‡è¦

- **TD does not follow the gradient of any objective function**  , TD æ²¡æœ‰éµå¾ªç€ç›®æ ‡å‡½æ•°çš„æ¢¯åº¦
- This is why TD can diverge when off-policy or using  non-linear function approximation
- **Gradient TD** follows true gradient of projected Bellman error 

| On/Off-Policy | Algorithm                   | Table Lookup    | Linear          | Non-Linear      |
| ------------- | --------------------------- | --------------- | --------------- | --------------- |
| On-Policy     | MC<br />TD<br />Gradient TD | Y<br />Y<br />Y | Y<br />Y<br />Y | Y<br />N<br />Y |
| Off-Policy    | MC<br />TD<br />Gradient TD | Y<br />Y<br />Y | Y<br />N<br />Y | Y<br />N<br />Y |



#### Convergence of Control Algorithms

| Algorithm                   | Table Lookup    | Linear          | Non-Linear      |
| ------------- | --------------------------- | --------------- | --------------- |
| Monte-Carlo Control | Y            | (Y)    | N |
| Sarsa               | Y            | (Y) | N |
| Q-Learning | Y | N | N |
| Gradient Q-Learning | Y | Y | N |

(Y) = chatters around near-optimal value function è¡¨ç¤ºåœ¨æ¥è¿‘æœ€ä¼˜ä»·å€¼å‡½æ•°é™„è¿‘éœ‡è¡

- Control é—®é¢˜ï¼Œå¤§å¤šæ•°éƒ½èƒ½å¾—åˆ°è¾ƒå¥½çš„ç­–ç•¥ï¼Œä½†æ˜¯ç†è®ºä¸Šåªè¦å­˜åœ¨**å‡½æ•°è¿‘ä¼¼**ï¼Œå°±éƒ½ä¸æ˜¯ä¸¥æ ¼æ”¶æ•›çš„ï¼Œæ¯”è¾ƒå¸¸è§çš„æ˜¯åœ¨æœ€ä¼˜ç­–ç•¥ä¸Šä¸‹éœ‡è¡ï¼Œé€æ¸é€¼è¿‘ç„¶åçªç„¶æ¥ä¸€æ¬¡å‘æ•£ï¼Œå†é€æ¸é€¼è¿‘, ä½†æœ€ç»ˆå¾ˆéš¾æ”¶æ•›äºä¸€ç‚¹ã€‚



## Batch Methods

- **Gradient descent** is simple and appealing

- But it is not sample efficient   ä¹‹å‰éƒ½æ˜¯sampleä¸€ä¸ªæ•°æ®å°±ä¸€æ¬¡æ›´æ–°ï¼Œ æ•ˆç‡å¤ªä½

- Batch methods seek to find the best fitting value function

- Given the agentâ€™s experience (â€œtraining dataâ€)

  



### Least Squares Prediction

- åœ¨ä¸€éƒ¨åˆ†æ•°æ®é‡Œé¢**æ–¹å·®ä¹‹å’Œ**æœ€å°çš„ï¼Œ è€Œä¸æ˜¯é‡åˆ°ä¸€æ­¥å°±æ›´æ–°ä¸€æ­¥
- Given value function approximation $\hat v(s,\mathbf w) â‰ˆ v_Ï€(s)$  
- **experience** $\mathcal D$ consisting of âŸ¨state,valueâŸ© pairs  
   $\mathcal D = {âŸ¨s_1, v_1^Ï€âŸ©, âŸ¨s_2, v_2^Ï€âŸ©, ..., âŸ¨s_T , v_T^Ï€ âŸ©}$ 
- Which parameters $\mathbf w$ give the best fitting value fn $\hat v(s,\mathbf w)$ ?
- **Least squares** algorithms find parameter vector w minimising **sum-squared error** between vÌ‚ (st,ğ•¨)  and target values $v^Ï€_t$,

$$
\begin{align}
LS(\mathbf{w}) & = \sum^T_{t=1}(v_t^\pi-\hat{v}(s_t, \mathbf{w}))^2 \\
& = \mathbb{E}_\mathcal{D}[(v^\pi-\hat{v}(s, \mathbf{w}))^2] \\
\end{align}
$$





####  Stochastic Gradient Descent with Experience Replay

- Given **experience** consisting of âŸ¨state, valueâŸ© pairs  $\mathcal D = {âŸ¨s_1, v_1^Ï€âŸ©, âŸ¨s_2, v_2^Ï€âŸ©, ..., âŸ¨s_T , v_T^Ï€ âŸ©}$ 

- Repeat: 
  1. **Sample** state, value from experience :  $âŸ¨s,v^Ï€âŸ© âˆ¼ \mathcal D $
  2. Apply **SGD** update : $\Delta \mathbf w=Î±(v^Ï€ âˆ’ \hat v(s,\mathbf w))âˆ‡_{\mathbf w} \hat v(s,\mathbf w)$

- **Converges** to **least squares** solution $\mathbf w^\pi= \text{argmin}_\mathbf w LS(\mathbf w)$ 

éšæœºçš„ä» æ•°æ®é›†é‡Œé¢å–ï¼Œæ‰“ç ´äº†å‰åçŠ¶æ€ä¹‹é—´çš„**å…³è”æ€§**ï¼› ç›‘ç£å­¦ä¹ é‡Œé¢æ¯”è¾ƒå¼ºè°ƒæ•°æ®çš„éšæœºæ€§ï¼Œæ›´å®¹æ˜“æ”¶æ•›åˆ°å¥½çš„ç»“æœ



#### Experience Replay in Deep Q-Networks (DQN)

- TDæ–¹æ³•ç»“åˆDNNå¯èƒ½ä¸ä¼šæ”¶æ•›ï¼Œä½†DQNä½¿ç”¨ **experience replay**å’Œ**fixed Q-targets** èƒ½åšåˆ°æ•ˆæœå¾ˆå¥½ã€‚
- å¯¹äºmodelfreeçš„æƒ…å†µï¼Œ replayä¸€ä¸ªå¾ˆå¤§çš„ä½œç”¨å°±æ˜¯å­¦ä¹ modelï¼Œé‚£äº›<s,a,r,s'>çš„é‡‡æ ·æ˜¯å¾ˆæœ‰ä»·å€¼çš„ï¼›
- DQN uses **experience replay** and **fixed Q-targets**  
- Take action $a_t$ according to **Îµ-greedy** policy    é‡‡æ ·æ˜¯æ¢ç´¢æ€§ç­–ç•¥
- Store **transition** $(s_t,a_t,r_{t+1},s_{t+1})$ in **replay memory** $\mathcal D$       
  å­˜åˆ°bufferï¼Œ**æ³¨æ„è¿™ä¸ªæ—¶å€™æ²¡å­˜Q** ï¼Œå› ä¸ºQæ˜¯ç®—å‡ºæ¥çš„ï¼Œè¶Šæ¥è¶Šå‡†, ä¹Ÿæ²¡æœ‰å­˜ä¸‹ä¸€ä¸ªa ï¼Œå› ä¸ºaæ˜¯é€šè¿‡qmaxç®—å‡ºæ¥çš„
- Sample random **mini-batch** of transitions $(s,a,r,sâ€²)$ from $\mathcal D$   éšæœºé‡‡æ ·æ‰“ç ´äº†çŠ¶æ€ä¹‹é—´çš„è”ç³»
- Compute **Q-learning targets** w.r.t. old, **fixed parameters $w^âˆ’$**  
- Optimise **MSE** between Q-network(for predicting) and Q-learning targets    

$$
\mathcal L_i(w_i) = \mathbb E_{s,a,r,s'\sim\mathcal D_i} \left[\left( r+ \gamma \max_{a'}Q(s',a';w_i^-) -Q(s,a;w_i)  \right)^2 \right]
$$

- Using variant of **SGD**

- ç»´æŠ¤ä¸¤ä¸ªç¥ç»ç½‘ç»œDQN1ï¼ŒDQN2,ä¸€ä¸ªç½‘ç»œå›ºå®šå‚æ•°ä¸“é—¨ç”¨æ¥äº§ç”Ÿç›®æ ‡å€¼ï¼Œç›®æ ‡å€¼ç›¸å½“äºæ ‡ç­¾æ•°æ®ã€‚å¦ä¸€ä¸ªç½‘ç»œä¸“é—¨ç”¨æ¥æ›´æ–°å‚æ•°ã€‚
- ç¬¬äºŒä¸ªç¥ç»ç½‘ç»œä¼šæš‚æ—¶å†»ç»“å‚æ•°ï¼Œæˆ‘ä»¬ä»å†»ç»“å‚æ•°çš„ç½‘ç»œè€Œä¸æ˜¯ä»æ­£åœ¨æ›´æ–°å‚æ•°çš„ç½‘ç»œä¸­è·å–ç›®æ ‡å€¼ï¼Œè¿™æ ·å¢åŠ äº†ç®—æ³•çš„ç¨³å®šæ€§ã€‚ç»è¿‡ä¸€æ¬¡æ‰¹è®¡ç®—åï¼ŒæŠŠå†»ç»“å‚æ•°çš„ç½‘ç»œæ¢æˆæ›´æ–°çš„å‚æ•°å†æ¬¡å†»ç»“äº§ç”Ÿæ–°ä¸€æ¬¡è¿­ä»£æ—¶è¦ç”¨çš„ç›®æ ‡å€¼ã€‚
- åœ¨ä¼ ç»ŸQlearningä¸­ï¼Œæ ¹æ®å…¬å¼ï¼Œ æ›´æ–°ä¸€æ¬¡Q value çš„æ—¶å€™ï¼Œä¼šåŒæ—¶æ›´æ–° target çš„Qvalue , å› ä¸ºéƒ½æ˜¯æ›´æ–°çš„å‡½æ•°çš„å‚æ•°ï¼Œç„¶åå»ç®—å‡ºæ¥çš„ï¼› è€ŒDQNå®é™…ä¸Šæ˜¯èµ°äº†è‹¥å¹²æ­¥ï¼Œæ‰æ›´æ–°ä¸€æ¬¡ target DQNçš„å‚æ•°

 

##### python é‡Œé¢å®ç° DQN 

1. Experience replay  æ¯é‡‡æ ·ä¸€æ¬¡ï¼Œå°±å¾€  replay memory é‡Œé¢åŠ ï¼Œç„¶åæŠŠæ•´ä¸ªreplay memoryå†sampleå‡ºæ¥è®­ç»ƒ
2. å¯¹modelæ¥è¯´ï¼Œ ä¸€ä¸ªepisodeä¼štrainå¾ˆå¤šæ¬¡ï¼Œ target model æ˜¯æ¯ä¸ªepisode ç»“æŸæ‰æ›´æ–°ä¸€æ¬¡ï¼›targetä¸å¤ªä¼šä¹±è·‘
3. å› ä¸ºè®­ç»ƒæ˜¯é’ˆå¯¹ s,aï¼Œç”¨çš„targetæ–°å€¼ï¼Œ  Q(s,a) <= r + Q(s',a') æ‰æ¥è‡ª target modelï¼ŒQ(s)çš„å…¶ä»–åŠ¨ä½œå€¼è¿˜æ˜¯ç”¨modelæœ¬èº«çš„
4. å¯¹replay ï¼Œ å­˜å…¥çš„å°±æ˜¯ s,a,r,s'  ; æ‰€ä»¥æ¯æ¬¡ experience replayçš„æ—¶å€™ï¼Œå°±æ˜¯æŠ½æ ·å‡ºæ¥ä¸€äº›å†ç”¨æœ€æ–°çš„modelç®—ä¸€é q å€¼ï¼›æœ‰ä»·å€¼çš„å°±æ˜¯è¿™äº›  dynamics
5. å½“ç„¶è¿™äº› dynamics æ˜¯ç›®å‰ç­–ç•¥ä»¥åŠä¸€äº›éšæœº, é‡‡æ ·æ¥çš„, ä¸€ç›´å­¦ä¹Ÿä¸ä¸€å®šèƒ½æ‰¾åˆ°æœ€ä¼˜, å¦‚æœç­–ç•¥æ²¡æœ‰æ¨ç€agentå¾€æœ€ä¼˜çš„æ–¹å‘èµ°çš„è¯,  å…‰é éšæœºæ˜¯å¾ˆéš¾èµ°å‡ºæœ€ä¼˜çš„.



#### DDQN = Double DQN

a' å¯ä»¥æ¥è‡ª model ï¼Œ Q(s',a')æ¥è‡ª target model ,  å³é€‰æ‹©a'ä¸ç®—Q(s',a')æ”¾äº†ä¸ªç½‘ç»œ, è§£è€¦.



#### DQN in Atari

- End-to-end learning of values $Q(s,a)$ from pixels s
- Input state s is stack of raw pixels from last 4 frames
- Output is $Q(s,a)$ for 18 joystick/button positions
- Reward is change in score for that step 
- Network architecture and hyperparameters fixed across all games

![image-20190220200811646](/img/2019-03-02-Silver.assets/image-20190220200811646.png)





### Convergence of Control Algorithms

| Algorithm           | Table Lookup | Linear | Non-Linear |
| ------------------- | ------------ | ------ | ---------- |
| Monte-Carlo Control | Y            | (Y)    | N          |
| Sarsa               | Y            | (Y)    | N          |
| Q-Learning          | Y            | N      | N          |
| LSPI                | Y            | (Y)    | --         |

(Y) = chatters around near-optimal value function è¡¨ç¤ºåœ¨æœ€ä¼˜ä»·å€¼å‡½æ•°é™„è¿‘éœ‡è¡





### ~~Linear Least Squares Prediction~~

- æœ¬æ–¹æ³•åªé€‚ç”¨äºçº¿æ€§æ‹Ÿåˆï¼Œç›´æ¥æ±‚è§£æè§£ï¼Œå±€é™æ€§æ¯”è¾ƒå¤§,  å¯ä»¥ä¸å¤ªçœ‹

- **Experience replay** finds least squares solution
- But it may take many iterations
- Using linear value function approximation $\hat v(s, \mathbf w) = \mathbf x(s)^âŠ¤ \mathbf w$
- We can solve the least squares solution directly   ç›´æ¥æ±‚è§£æè§£ 
- At minimum of $LS(\mathbf w)$, the expected update must be zero

$$
\begin{align}
\mathbb E_{\mathcal D}[\Delta \mathbf w] &= 0
\\ \alpha\sum_{t=1}^T \mathbf x(s_t)(v_t^\pi - \mathbf x(s_t)^\top\mathbf w) & =0
\\ \sum_{t=1}^T \mathbf x(s_t)v_t^\pi & = \sum_{t=1}^T \mathbf x(s_t) \mathbf x(s_t)^\top\mathbf w 
\\ \mathbf w &= \left( \sum_{t=1}^T \mathbf x(s_t) \mathbf x(s_t)^\top  \right)^{-1} \sum_{t=1}^T \mathbf x(s_t)v_t^\pi
\end{align}
$$

- For N features, direct solution time is $O(N^3)$
- Incremental solution time is $O(N^2)$ using Shermann-Morrison



#### Linear Least Squares Prediction Algorithms

- We do not know true values $v_t^Ï€$
- In practice, our â€œtraining dataâ€ must use noisy or biased samples of $v_t^Ï€$
  - **LSMC** Least Squares Monte-Carlo uses return 
    â€‹   $v_t^Ï€ â‰ˆ G_t$
  - **LSTD** Least Squares Temporal-Difference uses TD target 
    â€‹   $v_t^Ï€ â‰ˆR_{t+1}+Î³ \hat v(S_{t+1}, \mathbf w) $
  - **LSTD(Î»)** Least Squares TD(Î») uses Î»-return 
    â€‹   $v_t^Ï€ â‰ˆ G_t^Î»$
- In each case solve directly for **fixed point** of MC / TD / TD(Î») 
- LSMC

$$
\begin{align} 0 & = \sum_{t=1}^T\alpha \left(G_t - \hat v(S_t,\mathbf w) \right)\mathbf x(S_t)
\\ \mathbf w &= \left(\sum_{t=1}^T \mathbf x(S_t)\mathbf x(S_t)^\top\right)^{-1} \sum_{t=1}^T \mathbf x(S_t)G_t
\end{align}
$$

- LSTD 

$$
\begin{align} 0 & = \sum_{t=1}^T\alpha \left(R_{t+1}+ \gamma \hat v(S_{t+1},\mathbf w) - \hat v(S_t,\mathbf w) \right)\mathbf x(S_t)
\\ \mathbf w &= \left(\sum_{t=1}^T \mathbf x(S_t)(\mathbf x(S_t)- \gamma \mathbf x(S_{t+1}))^\top\right)^{-1} \sum_{t=1}^T \mathbf w(S_t)R_{t+1}
\end{align}
$$

- LSTD(Î»)

$$
\begin{align} 0 & = \sum_{t=1}^T\alpha \delta_t E_t
\\ \mathbf w &= \left(\sum_{t=1}^T  E_t (\mathbf x(S_t)- \gamma \mathbf x(S_{t+1}))^\top\right)^{-1} \sum_{t=1}^T   E_t R_{t+1}
\end{align}
$$




##### Convergence of Linear Least Squares Prediction Algorithms

| On/Off-Policy | Algorithm                      | Table Lookup           | Linear                 | Non-Linear               |
| ------------- | ------------------------------ | ---------------------- | ---------------------- | ------------------------ |
| On-Policy     | MC<br />LSMC<br />TD<br />LSTD | Y<br />Y<br />Y<br />Y | Y<br />Y<br />Y<br />Y | Y<br />--<br />N<br />-- |
| Off-Policy    | MC<br />LSMC<br />TD<br />LSTD | Y<br />Y<br />Y<br />Y | Y<br />Y<br />N<br />Y | Y<br />--<br />N<br />-- |





### Least Squares Control

#### Least Squares Policy Iteration   LSPI

- Policy evaluation: Policy evaluation by **least squares Q-learning**
- Policy improvement: Greedy policy improvement



#### Least Squares Action-Value Function Approximation

- Approximate action-value function $q_Ï€(s,a)$ 
- using linear combination of features $\mathbf x(s,a)$ :  $\hat q(s, a,\mathbf  w) =\mathbf x(s, a)^âŠ¤\mathbf w â‰ˆ q_Ï€(s, a) $
- Minimise least squares error between $\hat q(s, a,\mathbf w)$ and $q_Ï€(s, a)$ 
- from experience generated using policy Ï€
- consisting of âŸ¨(state, action), valueâŸ© pairs  $\mathcal D = {âŸ¨(s_1, a_1), v_1^Ï€âŸ©, âŸ¨(s_2, a_2), v_2^Ï€âŸ©, ..., âŸ¨(s_T , a_T ), v_T^Ï€ âŸ©}  $  



#### Least Squares Control

- For policy evaluation, we want to efficiently use all experience 
- For control, we also want to improve the policy
- This experience is generated from many policies
- So to evaluate $q_Ï€(S,A)$ we must learn **off-policy** 

**è¿™é‡Œè¯´æ˜ï¼Œ off-policy çš„ä¸¤ä¸ªæ–¹æ¡ˆï¼Œä¸€ä¸ªæ¨¡ä»¿Q-learning ä¸€ä¸ªæ˜¯importa sampling**

- We use the same idea as Q-learning:   **ä»target policy é‡Œé¢å– a'**
  - Use experience generated by old policy $S_t,A_t,R_{t+1},S_{t+1} âˆ¼Ï€_{old}$
  - Consider alternative successor action $Aâ€² = Ï€_{new} (S_{t+1})$ 
  - Update $\hat q(S_t , A_t ,\mathbf w)$ towards value of alternative action $R_{t+1} + Î³\hat q(S_{t+1}, Aâ€²,\mathbf w))$ 



##### Least Squares Q-Learning

- Consider the following linear Q-learning update

$$
\delta = R_{t+1} + \gamma \hat q(S_{t+1},\pi(S_{t+1}),\mathbf w) - \hat q(S_t,A_t,\mathbf w) 
\\ \Delta \mathbf w= \alpha \delta \mathbf x(S_t,A_t)
$$

- LSTDQ algorithm: solve for total update = zero

$$
0 = \sum_{t=1}^T \alpha(R{t+1}+\gamma \hat q(S_{t+1},\pi(S_{t+1},\mathbf w) - \hat q(S_t,A_t,\mathbf w)) \mathbf x(S_t,A_t)
\\ \mathbf w = \left(\sum_{t=1}^T \mathbf x(S_t,A_t)\Big(\mathbf x(S_t,A_t)-\gamma \mathbf x(S_{t+1},\pi(S_{t+1})) \Big)^\top \right) \sum_{t=1}^T \mathbf x(S_t,A_t)R_{t+1}
$$



##### Least Squares Policy Iteration Algorithm

- The following pseudocode uses LSTDQ for policy evaluation
- It repeatedly re-evaluates experience $\mathcal D$ with different policies

> function LSPI-TD($\mathcal D, Ï€_0$)   
> â€‹ $Ï€â€² â† Ï€_0$   
> â€‹ repeat   
> â€‹   $Ï€ â† Ï€â€²$  
>     Q â† LSTDQ($Ï€,\mathcal D$)   
> â€‹   for all $s âˆˆ\mathcal S$ do   
> â€‹     $Ï€â€²(s) â† \text{argmax}_{aâˆˆA} Q(s, a)$   
> â€‹   end for   
> â€‹ until $(Ï€ â‰ˆ Ï€â€²)$   
> â€‹ return Ï€   
> end function      
















